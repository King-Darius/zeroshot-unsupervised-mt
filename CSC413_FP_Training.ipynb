{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VhYGAHMKZJhV"
      },
      "source": [
        "## Train Loop\n",
        "\n",
        "PyTorch training for GAN:\n",
        "[PyTorch blog](https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ht8UL3hExpcQ",
        "outputId": "40e83aa4-7ae5-46e5-dc46-03bcc8306afd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "119547\n"
          ]
        }
      ],
      "source": [
        "print(tokenizer.vocab_size)\n",
        "real_decoder = Decoder(tokenizer.vocab_size)\n",
        "transformer = Transformer(tokenizer.vocab_size) # temp vocab size\n",
        "translate = Translator(hidden=512)\n",
        "discriminator = Discriminator()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "XFinB2ZTMh7B",
        "outputId": "9f98e44a-51a2-4c4f-fed1-6025e3dae297"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(\"L’algèbre linéaire est la branche des mathématiques qui s'intéresse aux espaces vectoriels et aux transformations linéaires, formalisation générale des théories des systèmes d'équations linéaires\", tensor([  101,   149,   100, 10164, 10240, 13340, 13724, 11614, 11246, 16556,\n",
            "        10176, 10109, 58760, 10139, 71777, 10355,   187,   112, 26391, 20017,\n",
            "        12818, 10754, 84355, 46514, 19428, 10107, 10131, 10754, 44510, 10107,\n",
            "        11614, 11246, 29194,   117, 23129, 20312, 28274, 10139, 38914, 10107,\n",
            "        10139, 48273,   172,   112,   263, 32973, 15024, 11614, 11246, 29194,\n",
            "          102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0]))\n",
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "Epoch 1:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\ttrain loss (decoder)   : 11.089984893798828\n",
            "\ttrain loss (generator) : 10.168094635009766\n",
            "\ttrain loss (discrim)   : 0.7588640451431274\n",
            "\ttrain loss (translator): 0.7372363805770874\n",
            "Epoch 2:\n",
            "\ttrain loss (decoder)   : 10.332171440124512\n",
            "\ttrain loss (generator) : 8.800978660583496\n",
            "\ttrain loss (discrim)   : 0.6262960433959961\n",
            "\ttrain loss (translator): 0.5939028859138489\n",
            "Epoch 3:\n",
            "\ttrain loss (decoder)   : 9.939229965209961\n",
            "\ttrain loss (generator) : 8.120532989501953\n",
            "\ttrain loss (discrim)   : 0.49083539843559265\n",
            "\ttrain loss (translator): 0.44378432631492615\n",
            "Epoch 4:\n",
            "\ttrain loss (decoder)   : 9.457409858703613\n",
            "\ttrain loss (generator) : 7.603317737579346\n",
            "\ttrain loss (discrim)   : 0.342607706785202\n",
            "\ttrain loss (translator): 0.2843707799911499\n",
            "Epoch 5:\n",
            "\ttrain loss (decoder)   : 9.061543464660645\n",
            "\ttrain loss (generator) : 6.988492488861084\n",
            "\ttrain loss (discrim)   : 0.23246333003044128\n",
            "\ttrain loss (translator): 0.17071151733398438\n",
            "Epoch 6:\n",
            "\ttrain loss (decoder)   : 8.654925346374512\n",
            "\ttrain loss (generator) : 6.379732131958008\n",
            "\ttrain loss (discrim)   : 0.15126635134220123\n",
            "\ttrain loss (translator): 0.0921720638871193\n",
            "Epoch 7:\n",
            "\ttrain loss (decoder)   : 8.301504135131836\n",
            "\ttrain loss (generator) : 6.1003336906433105\n",
            "\ttrain loss (discrim)   : 0.10820838809013367\n",
            "\ttrain loss (translator): 0.05235939100384712\n",
            "Epoch 8:\n",
            "\ttrain loss (decoder)   : 8.03753662109375\n",
            "\ttrain loss (generator) : 5.443886756896973\n",
            "\ttrain loss (discrim)   : 0.0875999927520752\n",
            "\ttrain loss (translator): 0.033998869359493256\n",
            "Epoch 9:\n",
            "\ttrain loss (decoder)   : 7.737112045288086\n",
            "\ttrain loss (generator) : 5.023810863494873\n",
            "\ttrain loss (discrim)   : 0.07359734177589417\n",
            "\ttrain loss (translator): 0.022502431645989418\n",
            "Epoch 10:\n",
            "\ttrain loss (decoder)   : 7.524195194244385\n",
            "\ttrain loss (generator) : 4.797091484069824\n",
            "\ttrain loss (discrim)   : 0.06613152474164963\n",
            "\ttrain loss (translator): 0.01708933711051941\n",
            "Epoch 11:\n",
            "\ttrain loss (decoder)   : 7.197145462036133\n",
            "\ttrain loss (generator) : 4.276239395141602\n",
            "\ttrain loss (discrim)   : 0.0597781278192997\n",
            "\ttrain loss (translator): 0.012813476845622063\n",
            "Epoch 12:\n",
            "\ttrain loss (decoder)   : 6.83424186706543\n",
            "\ttrain loss (generator) : 3.9649605751037598\n",
            "\ttrain loss (discrim)   : 0.05514352768659592\n",
            "\ttrain loss (translator): 0.010144397616386414\n",
            "Epoch 13:\n",
            "\ttrain loss (decoder)   : 6.576509475708008\n",
            "\ttrain loss (generator) : 3.7274224758148193\n",
            "\ttrain loss (discrim)   : 0.053114667534828186\n",
            "\ttrain loss (translator): 0.009984344244003296\n",
            "Epoch 14:\n",
            "\ttrain loss (decoder)   : 6.153345584869385\n",
            "\ttrain loss (generator) : 3.5787672996520996\n",
            "\ttrain loss (discrim)   : 0.04971659928560257\n",
            "\ttrain loss (translator): 0.008457906544208527\n",
            "Epoch 15:\n",
            "\ttrain loss (decoder)   : 6.029392719268799\n",
            "\ttrain loss (generator) : 3.0749895572662354\n",
            "\ttrain loss (discrim)   : 0.046246033161878586\n",
            "\ttrain loss (translator): 0.006849261000752449\n",
            "Epoch 16:\n",
            "\ttrain loss (decoder)   : 5.643759250640869\n",
            "\ttrain loss (generator) : 3.1940271854400635\n",
            "\ttrain loss (discrim)   : 0.04563632979989052\n",
            "\ttrain loss (translator): 0.00800974853336811\n",
            "Epoch 17:\n",
            "\ttrain loss (decoder)   : 5.424126148223877\n",
            "\ttrain loss (generator) : 2.749621629714966\n",
            "\ttrain loss (discrim)   : 0.04097124934196472\n",
            "\ttrain loss (translator): 0.005079046357423067\n",
            "Epoch 18:\n",
            "\ttrain loss (decoder)   : 5.122358798980713\n",
            "\ttrain loss (generator) : 2.4842491149902344\n",
            "\ttrain loss (discrim)   : 0.04038258269429207\n",
            "\ttrain loss (translator): 0.006148286629468203\n",
            "Epoch 19:\n",
            "\ttrain loss (decoder)   : 4.892360210418701\n",
            "\ttrain loss (generator) : 1.9214640855789185\n",
            "\ttrain loss (discrim)   : 0.03754379227757454\n",
            "\ttrain loss (translator): 0.004906090442091227\n",
            "Epoch 20:\n",
            "\ttrain loss (decoder)   : 4.44089937210083\n",
            "\ttrain loss (generator) : 1.7822765111923218\n",
            "\ttrain loss (discrim)   : 0.03648407757282257\n",
            "\ttrain loss (translator): 0.005384297575801611\n",
            "Epoch 21:\n",
            "\ttrain loss (decoder)   : 4.122931957244873\n",
            "\ttrain loss (generator) : 1.9187339544296265\n",
            "\ttrain loss (discrim)   : 0.03363824263215065\n",
            "\ttrain loss (translator): 0.004031533841043711\n",
            "Epoch 22:\n",
            "\ttrain loss (decoder)   : 3.958409547805786\n",
            "\ttrain loss (generator) : 1.4895203113555908\n",
            "\ttrain loss (discrim)   : 0.03206181153655052\n",
            "\ttrain loss (translator): 0.003938855137676001\n",
            "Epoch 23:\n",
            "\ttrain loss (decoder)   : 3.703050374984741\n",
            "\ttrain loss (generator) : 1.4236483573913574\n",
            "\ttrain loss (discrim)   : 0.03120930679142475\n",
            "\ttrain loss (translator): 0.004487935919314623\n",
            "Epoch 24:\n",
            "\ttrain loss (decoder)   : 3.4495937824249268\n",
            "\ttrain loss (generator) : 1.2365272045135498\n",
            "\ttrain loss (discrim)   : 0.028352392837405205\n",
            "\ttrain loss (translator): 0.0029850457794964314\n",
            "Epoch 25:\n",
            "\ttrain loss (decoder)   : 3.463207483291626\n",
            "\ttrain loss (generator) : 1.3275389671325684\n",
            "\ttrain loss (discrim)   : 0.02795594185590744\n",
            "\ttrain loss (translator): 0.00388769106939435\n",
            "Epoch 26:\n",
            "\ttrain loss (decoder)   : 3.023630380630493\n",
            "\ttrain loss (generator) : 0.9598378539085388\n",
            "\ttrain loss (discrim)   : 0.025747543200850487\n",
            "\ttrain loss (translator): 0.002939625410363078\n",
            "Epoch 27:\n",
            "\ttrain loss (decoder)   : 2.8401756286621094\n",
            "\ttrain loss (generator) : 0.5216966271400452\n",
            "\ttrain loss (discrim)   : 0.02474427968263626\n",
            "\ttrain loss (translator): 0.003176980884745717\n",
            "Epoch 28:\n",
            "\ttrain loss (decoder)   : 2.5480191707611084\n",
            "\ttrain loss (generator) : 0.48440250754356384\n",
            "\ttrain loss (discrim)   : 0.022739196196198463\n",
            "\ttrain loss (translator): 0.0023277585860341787\n",
            "Epoch 29:\n",
            "\ttrain loss (decoder)   : 2.4081027507781982\n",
            "\ttrain loss (generator) : 0.4745723605155945\n",
            "\ttrain loss (discrim)   : 0.021907826885581017\n",
            "\ttrain loss (translator): 0.0026009627617895603\n",
            "Epoch 30:\n",
            "\ttrain loss (decoder)   : 2.0525906085968018\n",
            "\ttrain loss (generator) : 0.48994114995002747\n",
            "\ttrain loss (discrim)   : 0.020334841683506966\n",
            "\ttrain loss (translator): 0.002057199366390705\n",
            "Epoch 31:\n",
            "\ttrain loss (decoder)   : 1.9608882665634155\n",
            "\ttrain loss (generator) : 0.36308208107948303\n",
            "\ttrain loss (discrim)   : 0.01954992674291134\n",
            "\ttrain loss (translator): 0.002255710307508707\n",
            "Epoch 32:\n",
            "\ttrain loss (decoder)   : 1.6785012483596802\n",
            "\ttrain loss (generator) : 0.35114845633506775\n",
            "\ttrain loss (discrim)   : 0.01811610907316208\n",
            "\ttrain loss (translator): 0.001736269099637866\n",
            "Epoch 33:\n",
            "\ttrain loss (decoder)   : 1.548817753791809\n",
            "\ttrain loss (generator) : 0.3742920160293579\n",
            "\ttrain loss (discrim)   : 0.01755714789032936\n",
            "\ttrain loss (translator): 0.002063723746687174\n",
            "Epoch 34:\n",
            "\ttrain loss (decoder)   : 1.4659537076950073\n",
            "\ttrain loss (generator) : 0.4146922826766968\n",
            "\ttrain loss (discrim)   : 0.016263099387288094\n",
            "\ttrain loss (translator): 0.0015827773604542017\n",
            "Epoch 35:\n",
            "\ttrain loss (decoder)   : 1.4318307638168335\n",
            "\ttrain loss (generator) : 0.35183849930763245\n",
            "\ttrain loss (discrim)   : 0.015910450369119644\n",
            "\ttrain loss (translator): 0.002017234917730093\n",
            "Epoch 36:\n",
            "\ttrain loss (decoder)   : 1.3838253021240234\n",
            "\ttrain loss (generator) : 0.27012208104133606\n",
            "\ttrain loss (discrim)   : 0.01433536782860756\n",
            "\ttrain loss (translator): 0.0011512355413287878\n",
            "Epoch 37:\n",
            "\ttrain loss (decoder)   : 1.3240814208984375\n",
            "\ttrain loss (generator) : 0.2626291513442993\n",
            "\ttrain loss (discrim)   : 0.014734218828380108\n",
            "\ttrain loss (translator): 0.0022637234069406986\n",
            "Epoch 38:\n",
            "\ttrain loss (decoder)   : 1.1575456857681274\n",
            "\ttrain loss (generator) : 0.2229704111814499\n",
            "\ttrain loss (discrim)   : 0.012763001956045628\n",
            "\ttrain loss (translator): 0.0009107152000069618\n",
            "Epoch 39:\n",
            "\ttrain loss (decoder)   : 0.9726645946502686\n",
            "\ttrain loss (generator) : 0.22252902388572693\n",
            "\ttrain loss (discrim)   : 0.013645054772496223\n",
            "\ttrain loss (translator): 0.002436893992125988\n",
            "Epoch 40:\n",
            "\ttrain loss (decoder)   : 0.9227433800697327\n",
            "\ttrain loss (generator) : 0.227665975689888\n",
            "\ttrain loss (discrim)   : 0.011436711996793747\n",
            "\ttrain loss (translator): 0.0007873087306506932\n",
            "Epoch 41:\n",
            "\ttrain loss (decoder)   : 0.8977555632591248\n",
            "\ttrain loss (generator) : 0.2755582332611084\n",
            "\ttrain loss (discrim)   : 0.012132135219871998\n",
            "\ttrain loss (translator): 0.0020672702230513096\n",
            "Epoch 42:\n",
            "\ttrain loss (decoder)   : 0.8147554993629456\n",
            "\ttrain loss (generator) : 0.2610050439834595\n",
            "\ttrain loss (discrim)   : 0.01041262224316597\n",
            "\ttrain loss (translator): 0.0008443816914223135\n",
            "Epoch 43:\n",
            "\ttrain loss (decoder)   : 0.7610060572624207\n",
            "\ttrain loss (generator) : 0.24846194684505463\n",
            "\ttrain loss (discrim)   : 0.010644380003213882\n",
            "\ttrain loss (translator): 0.0015886530745774508\n",
            "Epoch 44:\n",
            "\ttrain loss (decoder)   : 0.7920525074005127\n",
            "\ttrain loss (generator) : 0.24384984374046326\n",
            "\ttrain loss (discrim)   : 0.009471787139773369\n",
            "\ttrain loss (translator): 0.0008559791604056954\n",
            "Epoch 45:\n",
            "\ttrain loss (decoder)   : 0.7869777679443359\n",
            "\ttrain loss (generator) : 0.2107684314250946\n",
            "\ttrain loss (discrim)   : 0.009825986810028553\n",
            "\ttrain loss (translator): 0.0016633907798677683\n",
            "Epoch 46:\n",
            "\ttrain loss (decoder)   : 0.6506105065345764\n",
            "\ttrain loss (generator) : 0.1925617754459381\n",
            "\ttrain loss (discrim)   : 0.008494781330227852\n",
            "\ttrain loss (translator): 0.0007149020675569773\n",
            "Epoch 47:\n",
            "\ttrain loss (decoder)   : 0.5872969031333923\n",
            "\ttrain loss (generator) : 0.23286937177181244\n",
            "\ttrain loss (discrim)   : 0.008740410208702087\n",
            "\ttrain loss (translator): 0.0013670421903952956\n",
            "Epoch 48:\n",
            "\ttrain loss (decoder)   : 0.5109133124351501\n",
            "\ttrain loss (generator) : 0.21792961657047272\n",
            "\ttrain loss (discrim)   : 0.00785878300666809\n",
            "\ttrain loss (translator): 0.0008429913432337344\n",
            "Epoch 49:\n",
            "\ttrain loss (decoder)   : 0.48078492283821106\n",
            "\ttrain loss (generator) : 0.1961076855659485\n",
            "\ttrain loss (discrim)   : 0.007830742746591568\n",
            "\ttrain loss (translator): 0.0011757363099604845\n",
            "Epoch 50:\n",
            "\ttrain loss (decoder)   : 0.4555075466632843\n",
            "\ttrain loss (generator) : 0.18625564873218536\n",
            "\ttrain loss (discrim)   : 0.007040388882160187\n",
            "\ttrain loss (translator): 0.0006975313299335539\n",
            "Epoch 51:\n",
            "\ttrain loss (decoder)   : 0.41511988639831543\n",
            "\ttrain loss (generator) : 0.13621048629283905\n",
            "\ttrain loss (discrim)   : 0.007074202876538038\n",
            "\ttrain loss (translator): 0.0010508222039788961\n",
            "Epoch 52:\n",
            "\ttrain loss (decoder)   : 0.3922153413295746\n",
            "\ttrain loss (generator) : 0.13728167116641998\n",
            "\ttrain loss (discrim)   : 0.006380431819707155\n",
            "\ttrain loss (translator): 0.0006297025829553604\n",
            "Epoch 53:\n",
            "\ttrain loss (decoder)   : 0.4233005940914154\n",
            "\ttrain loss (generator) : 0.12482696771621704\n",
            "\ttrain loss (discrim)   : 0.006421795580536127\n",
            "\ttrain loss (translator): 0.0009547190857119858\n",
            "Epoch 54:\n",
            "\ttrain loss (decoder)   : 0.3353160321712494\n",
            "\ttrain loss (generator) : 0.12886305153369904\n",
            "\ttrain loss (discrim)   : 0.005810329224914312\n",
            "\ttrain loss (translator): 0.0005837708013132215\n",
            "Epoch 55:\n",
            "\ttrain loss (decoder)   : 0.3688869774341583\n",
            "\ttrain loss (generator) : 0.13941380381584167\n",
            "\ttrain loss (discrim)   : 0.00603501359000802\n",
            "\ttrain loss (translator): 0.0010657748207449913\n",
            "Epoch 56:\n",
            "\ttrain loss (decoder)   : 0.3815331757068634\n",
            "\ttrain loss (generator) : 0.12202157825231552\n",
            "\ttrain loss (discrim)   : 0.005239027086645365\n",
            "\ttrain loss (translator): 0.000475780077977106\n",
            "Epoch 57:\n",
            "\ttrain loss (decoder)   : 0.31606313586235046\n",
            "\ttrain loss (generator) : 0.10166311264038086\n",
            "\ttrain loss (discrim)   : 0.005654849112033844\n",
            "\ttrain loss (translator): 0.0011273862328380346\n",
            "Epoch 58:\n",
            "\ttrain loss (decoder)   : 0.27588555216789246\n",
            "\ttrain loss (generator) : 0.12452742457389832\n",
            "\ttrain loss (discrim)   : 0.004804972093552351\n",
            "\ttrain loss (translator): 0.0004585387068800628\n",
            "Epoch 59:\n",
            "\ttrain loss (decoder)   : 0.24247272312641144\n",
            "\ttrain loss (generator) : 0.13161729276180267\n",
            "\ttrain loss (discrim)   : 0.005218034610152245\n",
            "\ttrain loss (translator): 0.0010837893933057785\n",
            "Epoch 60:\n",
            "\ttrain loss (decoder)   : 0.26942354440689087\n",
            "\ttrain loss (generator) : 0.10764284431934357\n",
            "\ttrain loss (discrim)   : 0.004374078009277582\n",
            "\ttrain loss (translator): 0.00039712025318294764\n",
            "Epoch 61:\n",
            "\ttrain loss (decoder)   : 0.28772851824760437\n",
            "\ttrain loss (generator) : 0.11078443378210068\n",
            "\ttrain loss (discrim)   : 0.004852992482483387\n",
            "\ttrain loss (translator): 0.0010702944127842784\n",
            "Epoch 62:\n",
            "\ttrain loss (decoder)   : 0.32922300696372986\n",
            "\ttrain loss (generator) : 0.11991848796606064\n",
            "\ttrain loss (discrim)   : 0.0040327925235033035\n",
            "\ttrain loss (translator): 0.0003894051187671721\n",
            "Epoch 63:\n",
            "\ttrain loss (decoder)   : 0.23403416574001312\n",
            "\ttrain loss (generator) : 0.08960369229316711\n",
            "\ttrain loss (discrim)   : 0.004626040812581778\n",
            "\ttrain loss (translator): 0.0011587459594011307\n",
            "Epoch 64:\n",
            "\ttrain loss (decoder)   : 0.23221254348754883\n",
            "\ttrain loss (generator) : 0.08210652321577072\n",
            "\ttrain loss (discrim)   : 0.0037066356744617224\n",
            "\ttrain loss (translator): 0.0003616447211243212\n",
            "Epoch 65:\n",
            "\ttrain loss (decoder)   : 0.21666200459003448\n",
            "\ttrain loss (generator) : 0.07719570398330688\n",
            "\ttrain loss (discrim)   : 0.004150560591369867\n",
            "\ttrain loss (translator): 0.0009647643892094493\n",
            "Epoch 66:\n",
            "\ttrain loss (decoder)   : 0.16630016267299652\n",
            "\ttrain loss (generator) : 0.08411850035190582\n",
            "\ttrain loss (discrim)   : 0.0034323441796004772\n",
            "\ttrain loss (translator): 0.00035609921906143427\n",
            "Epoch 67:\n",
            "\ttrain loss (decoder)   : 0.20539997518062592\n",
            "\ttrain loss (generator) : 0.08792475610971451\n",
            "\ttrain loss (discrim)   : 0.00384900183416903\n",
            "\ttrain loss (translator): 0.0009166041272692382\n",
            "Epoch 68:\n",
            "\ttrain loss (decoder)   : 0.2920331656932831\n",
            "\ttrain loss (generator) : 0.08431979268789291\n",
            "\ttrain loss (discrim)   : 0.0032251582015305758\n",
            "\ttrain loss (translator): 0.0003936025022994727\n",
            "Epoch 69:\n",
            "\ttrain loss (decoder)   : 0.2632243037223816\n",
            "\ttrain loss (generator) : 0.06552600115537643\n",
            "\ttrain loss (discrim)   : 0.003530727932229638\n",
            "\ttrain loss (translator): 0.000826433184556663\n",
            "Epoch 70:\n",
            "\ttrain loss (decoder)   : 0.22077558934688568\n",
            "\ttrain loss (generator) : 0.06323590874671936\n",
            "\ttrain loss (discrim)   : 0.002911602146923542\n",
            "\ttrain loss (translator): 0.00029269553488120437\n",
            "Epoch 71:\n",
            "\ttrain loss (decoder)   : 0.22048532962799072\n",
            "\ttrain loss (generator) : 0.06219703331589699\n",
            "\ttrain loss (discrim)   : 0.003356343600898981\n",
            "\ttrain loss (translator): 0.0008596386760473251\n",
            "Epoch 72:\n",
            "\ttrain loss (decoder)   : 0.18890278041362762\n",
            "\ttrain loss (generator) : 0.06610655039548874\n",
            "\ttrain loss (discrim)   : 0.0027557332068681717\n",
            "\ttrain loss (translator): 0.00033745565451681614\n",
            "Epoch 73:\n",
            "\ttrain loss (decoder)   : 0.1539686769247055\n",
            "\ttrain loss (generator) : 0.05613904818892479\n",
            "\ttrain loss (discrim)   : 0.003130782162770629\n",
            "\ttrain loss (translator): 0.0008207246428355575\n",
            "Epoch 74:\n",
            "\ttrain loss (decoder)   : 0.14160342514514923\n",
            "\ttrain loss (generator) : 0.04296160861849785\n",
            "\ttrain loss (discrim)   : 0.002559828106313944\n",
            "\ttrain loss (translator): 0.0003197703917976469\n",
            "Epoch 75:\n",
            "\ttrain loss (decoder)   : 0.09971398860216141\n",
            "\ttrain loss (generator) : 0.049153707921504974\n",
            "\ttrain loss (discrim)   : 0.0028715303633362055\n",
            "\ttrain loss (translator): 0.0007303249440155923\n",
            "Epoch 76:\n",
            "\ttrain loss (decoder)   : 0.10660009831190109\n",
            "\ttrain loss (generator) : 0.04281860962510109\n",
            "\ttrain loss (discrim)   : 0.0023501154500991106\n",
            "\ttrain loss (translator): 0.0002696281881071627\n",
            "Epoch 77:\n",
            "\ttrain loss (decoder)   : 0.15410257875919342\n",
            "\ttrain loss (generator) : 0.034875232726335526\n",
            "\ttrain loss (discrim)   : 0.0027522773016244173\n",
            "\ttrain loss (translator): 0.0007654153741896152\n",
            "Epoch 78:\n",
            "\ttrain loss (decoder)   : 0.16227661073207855\n",
            "\ttrain loss (generator) : 0.038060709834098816\n",
            "\ttrain loss (discrim)   : 0.002167039318010211\n",
            "\ttrain loss (translator): 0.00023196754045784473\n",
            "Epoch 79:\n",
            "\ttrain loss (decoder)   : 0.09887224435806274\n",
            "\ttrain loss (generator) : 0.031423378735780716\n",
            "\ttrain loss (discrim)   : 0.002731023821979761\n",
            "\ttrain loss (translator): 0.000885062268935144\n",
            "Epoch 80:\n",
            "\ttrain loss (decoder)   : 0.13579313457012177\n",
            "\ttrain loss (generator) : 0.03296348452568054\n",
            "\ttrain loss (discrim)   : 0.0020257390569895506\n",
            "\ttrain loss (translator): 0.00022559058561455458\n",
            "Epoch 81:\n",
            "\ttrain loss (decoder)   : 0.20184434950351715\n",
            "\ttrain loss (generator) : 0.038089118897914886\n",
            "\ttrain loss (discrim)   : 0.0024459126871079206\n",
            "\ttrain loss (translator): 0.0007278703851625323\n",
            "Epoch 82:\n",
            "\ttrain loss (decoder)   : 0.11263766139745712\n",
            "\ttrain loss (generator) : 0.03362136706709862\n",
            "\ttrain loss (discrim)   : 0.0019136150367558002\n",
            "\ttrain loss (translator): 0.00023712973052170128\n",
            "Epoch 83:\n",
            "\ttrain loss (decoder)   : 0.1206367015838623\n",
            "\ttrain loss (generator) : 0.031214425340294838\n",
            "\ttrain loss (discrim)   : 0.0022537983022630215\n",
            "\ttrain loss (translator): 0.0006512774270959198\n",
            "Epoch 84:\n",
            "\ttrain loss (decoder)   : 0.1016983911395073\n",
            "\ttrain loss (generator) : 0.023040123283863068\n",
            "\ttrain loss (discrim)   : 0.001788727706298232\n",
            "\ttrain loss (translator): 0.00022369035286828876\n",
            "Epoch 85:\n",
            "\ttrain loss (decoder)   : 0.14565350115299225\n",
            "\ttrain loss (generator) : 0.01739218831062317\n",
            "\ttrain loss (discrim)   : 0.0021865007001906633\n",
            "\ttrain loss (translator): 0.0006910159718245268\n",
            "Epoch 86:\n",
            "\ttrain loss (decoder)   : 0.12367310374975204\n",
            "\ttrain loss (generator) : 0.021180450916290283\n",
            "\ttrain loss (discrim)   : 0.001670189667493105\n",
            "\ttrain loss (translator): 0.00020709623640868813\n",
            "Epoch 87:\n",
            "\ttrain loss (decoder)   : 0.08211853355169296\n",
            "\ttrain loss (generator) : 0.018291881307959557\n",
            "\ttrain loss (discrim)   : 0.0019516398897394538\n",
            "\ttrain loss (translator): 0.0005518507095985115\n",
            "Epoch 88:\n",
            "\ttrain loss (decoder)   : 0.07071124762296677\n",
            "\ttrain loss (generator) : 0.017078282311558723\n",
            "\ttrain loss (discrim)   : 0.0015676888870075345\n",
            "\ttrain loss (translator): 0.00019815667474176735\n",
            "Epoch 89:\n",
            "\ttrain loss (decoder)   : 0.05490438640117645\n",
            "\ttrain loss (generator) : 0.025501471012830734\n",
            "\ttrain loss (discrim)   : 0.001949751516804099\n",
            "\ttrain loss (translator): 0.0006416069227270782\n",
            "Epoch 90:\n",
            "\ttrain loss (decoder)   : 0.0560808889567852\n",
            "\ttrain loss (generator) : 0.021325135603547096\n",
            "\ttrain loss (discrim)   : 0.0014791472349315882\n",
            "\ttrain loss (translator): 0.00019631700706668198\n",
            "Epoch 91:\n",
            "\ttrain loss (decoder)   : 0.09757127612829208\n",
            "\ttrain loss (generator) : 0.019738957285881042\n",
            "\ttrain loss (discrim)   : 0.0017978642135858536\n",
            "\ttrain loss (translator): 0.0005712389247491956\n",
            "Epoch 92:\n",
            "\ttrain loss (decoder)   : 0.06089545413851738\n",
            "\ttrain loss (generator) : 0.018733784556388855\n",
            "\ttrain loss (discrim)   : 0.0013845280045643449\n",
            "\ttrain loss (translator): 0.00018033917876891792\n",
            "Epoch 93:\n",
            "\ttrain loss (decoder)   : 0.08200383186340332\n",
            "\ttrain loss (generator) : 0.018843041732907295\n",
            "\ttrain loss (discrim)   : 0.0015521370805799961\n",
            "\ttrain loss (translator): 0.0003964838688261807\n",
            "Epoch 94:\n",
            "\ttrain loss (decoder)   : 0.06145733222365379\n",
            "\ttrain loss (generator) : 0.01611945778131485\n",
            "\ttrain loss (discrim)   : 0.0013639060780405998\n",
            "\ttrain loss (translator): 0.0002364729152759537\n",
            "Epoch 95:\n",
            "\ttrain loss (decoder)   : 0.0525912344455719\n",
            "\ttrain loss (generator) : 0.0180159043520689\n",
            "\ttrain loss (discrim)   : 0.0014538936084136367\n",
            "\ttrain loss (translator): 0.0003673455212265253\n",
            "Epoch 96:\n",
            "\ttrain loss (decoder)   : 0.04122551158070564\n",
            "\ttrain loss (generator) : 0.01707887463271618\n",
            "\ttrain loss (discrim)   : 0.0013057211181148887\n",
            "\ttrain loss (translator): 0.00024634928558953106\n",
            "Epoch 97:\n",
            "\ttrain loss (decoder)   : 0.04489303007721901\n",
            "\ttrain loss (generator) : 0.014117339625954628\n",
            "\ttrain loss (discrim)   : 0.0013977454509586096\n",
            "\ttrain loss (translator): 0.0003763608983717859\n",
            "Epoch 98:\n",
            "\ttrain loss (decoder)   : 0.03851773217320442\n",
            "\ttrain loss (generator) : 0.016620561480522156\n",
            "\ttrain loss (discrim)   : 0.0012329055462032557\n",
            "\ttrain loss (translator): 0.00023537140805274248\n",
            "Epoch 99:\n",
            "\ttrain loss (decoder)   : 0.04743150994181633\n",
            "\ttrain loss (generator) : 0.013990260660648346\n",
            "\ttrain loss (discrim)   : 0.001355565618723631\n",
            "\ttrain loss (translator): 0.0003946412180084735\n",
            "Epoch 100:\n",
            "\ttrain loss (decoder)   : 0.05934547260403633\n",
            "\ttrain loss (generator) : 0.011633341200649738\n",
            "\ttrain loss (discrim)   : 0.001140328706242144\n",
            "\ttrain loss (translator): 0.00019841713947243989\n"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIYAAAB4CAYAAAA34Yr4AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAALcklEQVR4nO2db2wUZR7Hv8/M/uu2uy1KbW2peITTWs5QqC1d5KIxjeSMCU3uAjE5hZ6AOcU3fUVjTqK+aIyeMUFz+KbX5EgQPRNRMAZt7o3SKkG8Q9BrSc52m3ZLe9D9B93tzjz3YjpTClO6s+3T5dn7fZJJutN55vnN7Gef55mZZ56Hcc45COIGlHwHQNyekBiELSQGYQuJQdhCYhC2kBiELSQGYYsr3wFkg67rGBkZQSAQAGMs3+FIAecc8XgcVVVVUBTnv38pxBgZGUFNTU2+w5CScDiMVatWOU4nhRiBQACAcZDBYDDP0chBLBZDTU2Nde6cIoUYZvURDAbRfzmD+FQGzWvuRJFHzXNktz+5Vr3SNT7b/noabd2nMRq9lu9QChrpxCj2GoVcMqXlOZLCRjoxSnyGGIlUJs+RFDbSiWGWGCSGWKQTI2BVJSSGSKQTo9hrXInESQyhSCgGlRjLgXRiUFWyPEgnhllixKdIDJFIJ4Z5uUolhljkE8OsStIkhkikFYOqErFIJwZdlSwP0okRoDufy4J0YtBDtOVBWjGoxBCLhGIYt8Sv0lWJUCQUwygxpjWOVIaqE1HIJ4ZntjcitTPEIZ0YqsJQ5DaqE7pkFUdOYrz77ru499574fP5sGnTJnz77bfzbtvd3Q3G2JzF5/PlHDAw286gBqg4HItx9OhRtLe348CBA/juu++wfv16bN26FZcuXZo3TTAYxOjoqLUMDg4uKmiznUENUHE4FuOtt97Cnj170NbWhrq6Ohw6dAh+vx9dXV3zpmGMobKy0loqKipumUcqlUIsFpuzXI/ZzkhQG0MYjsRIp9M4c+YMWlpaZnegKGhpaUFvb++86RKJBFavXo2amhps27YN58+fv2U+nZ2dKC0ttZYb30IrodviwnEkxsTEBDRNu+kXX1FRgUgkYpvm/vvvR1dXF44dO4bDhw9D13Vs3rwZw8PD8+bT0dGBaDRqLeFweM7//dTGEI7wN9FCoRBCoZD1efPmzXjggQfw3nvv4bXXXrNN4/V64fV6592n1cYgMYThqMRYuXIlVFXF2NjYnPVjY2OorKzMah9utxsbNmzAxYsXnWQ9hxKP2SeD2hiicCSGx+NBQ0MDenp6rHW6rqOnp2dOqXArNE3DuXPncPfddzuL9DroeYl4HFcl7e3t2LlzJx566CE0NTXh7bffRjKZRFtbGwDgmWeeQXV1NTo7OwEAr776Kpqbm7F27VpMTk7ijTfewODgIHbv3p1z0CVeusElGsdi7NixA+Pj43j55ZcRiURQX1+Pzz//3GqQDg0NzRmo48qVK9izZw8ikQhWrFiBhoYGnDp1CnV1dTkH7bfuY1BVIgomw8jAsVgMpaWliEajCAaD+Fvvz/jTsfP4za8q8ZffN+Q7vNuSG8+ZU6R7VgIAfmp8CkdKMaw+GdTGEIaUYlCJIR4pxaBeXOKRUgyrxKCHaMKQUgzz6SqVGOKQUgy/VZVo0PXb/mpbSqQU4/p+n9emqToRgZRi+NwKzOEr6eVmMUgpBmNstp1BDVAhSCkGAPg91FlHJNKKUUwP0oQirRhmiUGP3sUgrRiBmSGXYlPTeY6kMJFWjNIiNwAgdo3EEIH8YtCQS0KQXowolRhCkF+MqySGCOQXg0oMIUgrRpDEEIq0Ysw2PkkMEUgrBpUYYpFWDGpjiEVaMcpmxIhPZZDR9DxHU3jIK4bfA1UxOmVcTqbzHE3hIa0YqsJwZ7EHAHApnspzNIWHtGIAQHnAGENjnMRYcqQW464ZMS7Fp/IcSeEhtRhUYoijIMSgNsbSI7UYdwWMgWQvxUiMpUZqMVatKAIA/GcimedICg+pxairMgYEuTiewBS9eLSkSC1GZdCHFX43NJ1jYCyR73AKCqnFYIxhXVUpAOD78JU8R1NYSC0GAPz6lysBAH8/M/9Iw4RzhE9LAQAffvghamtr4fP58OCDD+Kzzz7LKVg7ftewCh5VwT+Hozh53n7YasI5wqelOHXqFJ566ik8++yzOHv2LFpbW9Ha2ooffvhh0cEDwJ0lXvxhyy8AAPuOnMXBngH8a3gScerAsygcD+e4adMmNDY24p133gFgjAxcU1ODF198Efv3779p+x07diCZTOL48ePWuubmZtTX1+PQoUNZ5bnQ0IRT0xr+ePgM/vHv8dkDY4BbVawGqt/jQnnACw7jZSWVMfg9Koq9LrhUBpfC4FYVqAoDA6DpHKqqQNN0qAqD3+MCY0BG4/C6lZk8GBQGqDOv3mucQ2UMjAGaDijM2EbTOVQFUBiDebIVNpPPTBpFAXTd+OxW2cy5BRQFMLYEOLixD24cH5vJ1/wKFcZQVebD2rsCix7O0dEAsOa0FB0dHda6haal6O3tRXt7+5x1W7duxccffzxvPqlUCqnU7E2rG+cruRGfW0XXrkYc+34Eh/sG8fN/r2IikUI6o2Po8lUMXc7i4AqE325chT9vX7/o/TgS41bTUvz000+2aSKRiKNpLABjvpJXXnnFSWhgjKF1QzVaN1QbsSZSmEikMPTfq9A5x7VpDRPxNBgz+m8wBqSmdSRSGWR0Dk3nSGs6dJ1D5xycA6mMbv3q0xmjM5CiMEzP/M3BoXNYo/oojEHjHJxzqAqDzo2Sx6Uw6JxD4wCD8Ws30zDGwLmxH8aMfWR0HQxGycM5oHNulBAwSh9z/eyxz25XPXPTb7EIn5YiFzo6OuaUMrFY7KbJbBZiZYkXK0u8qK10XowSDsXIZVqKyspKx9NY3DhfiVmHLlSlELOY5yrnEcG5Q5qamvi+ffusz5qm8erqat7Z2Wm7/fbt2/mTTz45Z10oFOLPPfdc1nmGw2EOgJYclnA47PQr5pwb9aEj3n//fe71enl3dze/cOEC37t3Ly8rK+ORSIRzzvnTTz/N9+/fb23/9ddfc5fLxd98803+448/8gMHDnC3283PnTuXdZ6apvFwOMwnJydJkixlMM+VpmlOv2LOeQ5icM75wYMH+T333MM9Hg9vamrifX191v8eeeQRvnPnzjnbf/DBB/y+++7jHo+Hr1u3jp84cSKnYDnnPBqN5v3E3+5LNBrN+fyaSDEtxfWY1+fE/OR67+J6pH9WQojhtrxcvRVerxcvvfQSpqam0NfXh8bGRpw+fRqhUAic8wXXOd1+OdYt1X5VVYXL5brlDJTZIl1VQiwPVJUQtpAYhC0kBmELiUHYQmIQtkglxt69e+HxeMAYo8XBUlFRgYGBAUfnWioxvvzyS0xPT6O8vDzfoThCVdVlz7OsrAyPP/44AGB8fBwPP/wwksnsX8ySSozy8nK88MILeP755wHMdm2bj1AohC+++GI5QgNgdDGwi6m4uBhVVVWora1d8jxVVYXb7bY++3zGa5tbtmxBUVERHnvsMdTX1yOZTOLIkSNZ71caMcxuhS0tLVmnGR4exsjIiMCo5hKJRGz7P8RiMYyMjKC/v3/J89S0uW/gTU0ZQ0KcPHkSn376KZqbmzEwMIDi4mJ89dVXWe9XGjGu71Y4NDQEAAt2QonFYti9e/dyhJcVui5mrLDp6Zt7xKfTaei6jtdffx1PPPEExsfHMTo6mvU+pRHDZGxsDCdOnACwcFUSj8dtT9r/A4pifLWapuGjjz5CY2OjtS6r9KICW2rMboXffPON9Q7LQiWGqF+oDJjHvnr1auzatQv9/f1Ys2ZN1umlEcPj8WDjxo345JNPrKsSJ7+A+fYpA7kcp9/vB2CIkUgkEI1GsW3btuzzdJxjHgkEArhw4YJ10IstEdJpOYaBdHKc5lWJ+V7OHXfcgaNHj6K+vt66fM0GqR67L9SmIG7G5XLh0UcfxfHjxx3105Cqo45EDkuPVFUJsXyQGIQtJAZhC4lB2EJiELaQGIQtJAZhC4lB2EJiELaQGIQtJAZhy/8Avvy1NM1QdSQAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 100x100 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from torch import optim\n",
        "# https://huggingface.co/docs/transformers/main_classes/optimizer_schedules#transformers.Adafactor\n",
        "from transformers.optimization import Adafactor\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive\n",
        "\n",
        "context_length = 64\n",
        "\n",
        "# TODO: add device specific code\n",
        "\n",
        "def pad(tokens, context_length):\n",
        "  l = list(tokens)\n",
        "  for _ in range(context_length - len(tokens)):\n",
        "    l.append(tokenizer.pad_token_id)\n",
        "  return np.array(l)\n",
        "\n",
        "def train(real_decoder, transformer, discriminator, translate, # our four models\n",
        "          real_train, other_train, real_valid = None, other_valid = None,\n",
        "          epochs = 10, batch_size = 256, ckpt_path = None, ckpt_interval = 10):\n",
        "  batch_data = []\n",
        "\n",
        "  # yash\n",
        "  data_loader = [(real_train, other_train)]\n",
        "\n",
        "  criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
        "  criterion_binary = nn.BCEWithLogitsLoss()\n",
        "  mse = nn.MSELoss()\n",
        "\n",
        "  # r_optim = optim.RAdam(real_decoder.parameters())\n",
        "  # g_optim = optim.RAdam(generator.parameters())\n",
        "  r_optim = Adafactor(real_decoder.parameters())\n",
        "  g_optim = Adafactor(transformer.parameters())\n",
        "  t_optim = Adafactor(translate.parameters())\n",
        "  d_optim = Adafactor(discriminator.parameters())\n",
        "\n",
        "  r_iterations = len(real_train) // batch_size\n",
        "  o_iterations = len(other_train) // batch_size\n",
        "  # r_iterations = real_train.shape[0] // batch_size\n",
        "  # o_iterations = other_train.shape[0] // batch_size\n",
        "\n",
        "  n = min(r_iterations, o_iterations)\n",
        "\n",
        "  losses = []\n",
        "\n",
        "  for e in range(epochs):\n",
        "    r_epoch_loss = 0\n",
        "    g_epoch_loss = 0\n",
        "    d_epoch_loss = 0\n",
        "    t_epoch_loss = 0\n",
        "    # yash (every epoch: shuffle)\n",
        "    # data_loader = batcherize(train_data)\n",
        "    # data_loader.shuffle()\n",
        "    # random.shuffle(batches of the dataloader)\n",
        "    print(f'Epoch {e+1}:')\n",
        "    for i, (r_x, o_x) in enumerate(data_loader):\n",
        "      # r_x: (english sentence: str, CLIP embeddings: float[512], tokens: one-hots[num tokens])[batch_size]\n",
        "      # o_x: (fr sentence: str, tokens: one-hot[num tokens])[batch_size]\n",
        "      # if r_x.shape[0] < batch_size: # not full batch\n",
        "      #   break\n",
        "      if (i + 1) % 100 == 0:\n",
        "        print(f'Iteration {i+1} of {n}')\n",
        "\n",
        "      rx_clips = torch.tensor(np.array(list(map(lambda x: x[1], r_x))))\n",
        "      rx_toks = torch.tensor(np.array(list(map(lambda x: x[2].numpy(), r_x))))\n",
        "      ox_toks = torch.tensor(np.array(list(map(lambda x: x[1].numpy(), o_x))))\n",
        "\n",
        "      # ==============================\n",
        "      # == learn decoder\n",
        "      # ==============================\n",
        "      tgt_in = rx_toks[:, :-1]\n",
        "      tgt_expect = rx_toks[:, 1:]\n",
        "      r_mask = nn.Transformer.generate_square_subsequent_mask(context_length - 1)\n",
        "      r_output = real_decoder(rx_clips, tgt_in, tgt_mask=r_mask)\n",
        "      r_output = r_output.permute(0,2,1)\n",
        "      r_loss = criterion(r_output, tgt_expect)\n",
        "      r_epoch_loss += r_loss.item()\n",
        "\n",
        "      r_optim.zero_grad()\n",
        "      r_loss.backward(retain_graph=True)\n",
        "      r_optim.step()\n",
        "\n",
        "      # ==============================\n",
        "      # == self learn monolingual\n",
        "      # ==============================\n",
        "      # \"other\" generator self supervised\n",
        "      # https://jamesmccaffrey.wordpress.com/2022/09/09/simplest-transformer-seq-to-seq-example/\n",
        "      \n",
        "      src = ox_toks\n",
        "      tgt = src\n",
        "      tgt_in = tgt[:,:-1]\n",
        "      tgt_expect = tgt[:,1:]\n",
        "      t_mask = nn.Transformer.generate_square_subsequent_mask(context_length - 1)\n",
        "      # https://pytorch.org/tutorials/beginner/translation_transformer.html#seq2seq-network-using-transformer\n",
        "      tgt_attn_mask = (tgt_in == tokenizer.pad_token_id)\n",
        "      attn_mask = (src == tokenizer.pad_token_id)\n",
        "\n",
        "      output, other_embeddings = transformer(src, tgt_in, tgt_mask=t_mask, tp_mask=tgt_attn_mask, sp_mask=attn_mask) # [bs,seq,vocab]\n",
        "\n",
        "      # get preds shape to conform to tgt_expect\n",
        "      output = output.permute(0,2,1)  # now [bs, vocab, seq]\n",
        "\n",
        "      g_loss = criterion(output, tgt_expect)\n",
        "      g_epoch_loss += g_loss.item()\n",
        "\n",
        "      g_optim.zero_grad()\n",
        "      g_loss.backward(retain_graph=True)\n",
        "      g_optim.step()\n",
        "\n",
        "      # ==============================\n",
        "      # == learn discriminator\n",
        "      # ==============================\n",
        "      fake_embs, F_embs = translate(other_embeddings[:,-1,:])\n",
        "      real_embs = rx_clips[:,-1,:]\n",
        "      inputs = torch.cat([real_embs, fake_embs])\n",
        "      reals = torch.ones(2) #whatever batch_sizes will be\n",
        "      fakes = torch.zeros(16) # ^^\n",
        "      labels = torch.cat([reals,fakes]) #[n_1 + n_2,512]\n",
        "\n",
        "      d_outputs = discriminator(inputs) \n",
        "      d_loss = criterion_binary(d_outputs, labels)\n",
        "      d_epoch_loss += d_loss.item()\n",
        "      d_optim.zero_grad()\n",
        "      d_loss.backward(retain_graph=True)\n",
        "      d_optim.step()\n",
        "\n",
        "      # ==============================\n",
        "      # == learn translator\n",
        "      # ==============================\n",
        "      \n",
        "      t_outputs = discriminator(fake_embs)\n",
        "      t_loss = criterion_binary(t_outputs, fakes)\n",
        "      tl_loss = mse(F_embs, other_embeddings[:,-1,:]) # \"cycle GAN\" reconstruct fr embeddings\n",
        "      t_epoch_loss += t_loss.item()\n",
        "      t_optim.zero_grad()\n",
        "      t_loss.backward(retain_graph=True)\n",
        "      tl_loss.backward()\n",
        "      t_optim.step()\n",
        "\n",
        "\n",
        "    print(f'\\ttrain loss (decoder)   : {r_epoch_loss}')\n",
        "    print(f'\\ttrain loss (generator) : {g_epoch_loss}')\n",
        "    print(f'\\ttrain loss (discrim)   : {d_epoch_loss}')\n",
        "    print(f'\\ttrain loss (translator): {t_epoch_loss}')\n",
        "    losses.append(t_epoch_loss)\n",
        "    if ckpt_path is not None and e % ckpt_interval == 0:\n",
        "      state = {\n",
        "          'real_decoder_state': real_decoder.state_dict(),\n",
        "          'real_decoder_loss': r_epoch_loss,\n",
        "          'transformer_state': transformer.state_dict(),\n",
        "          'transformer_loss': t_epoch_loss,\n",
        "          'discriminator_state': discriminator.state_dict(),\n",
        "          'discriminator_loss': d_epoch_loss,\n",
        "          'translate_state': translate.state_dict(),\n",
        "          'translate_loss': t_epoch_loss,\n",
        "      }\n",
        "      torch.save(state, ckpt_path + f'/ckpt-epoch-{e}.pt')\n",
        "  # plt.figure(figsize=(1, 1))\n",
        "  # plt.subplot(1, 1, 1)\n",
        "  plt.xticks(np.arange(epochs))\n",
        "  plt.plot(losses)\n",
        "  plt.show()\n",
        "\n",
        "print(train_data_fr[0])\n",
        "# To disable checkpointing, comment the next two lines and remove the ckpt_path\n",
        "# parameter in the call to train().\n",
        "drive.mount('/content/gdrive')\n",
        "ckpt_path = '/content/gdrive/My Drive/CSC413/Project/'\n",
        "train(real_decoder, transformer, discriminator, translate, [train_data_en[0]] * 2, [train_data_fr[0]] * 16, epochs=100, ckpt_path=ckpt_path)\n",
        "# train(real_decoder, transformer, discriminator, translate, train_data_en, train_data_fr, epochs=100, ckpt_path=ckpt_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWWqYTznQPZb"
      },
      "source": [
        "### Overfit Inference"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "ckpt_path = '/content/gdrive/My Drive/CSC413/Project/ckpt-epoch-90.pt'\n",
        "checkpoint = torch.load(ckpt_path)\n",
        "\n",
        "real_decoder.load_state_dict(checkpoint['real_decoder_state'])\n",
        "transformer.load_state_dict(checkpoint['transformer_state'])\n",
        "translate.load_state_dict(checkpoint['translate_state'])\n",
        "# discriminator.load_state_dict(checkpoint['discriminator'])\n",
        "\n",
        "real_decoder.eval()\n",
        "transformer.eval()\n",
        "translate.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wLKZl0Iapikz",
        "outputId": "b53a6e61-c6b1-4585-a2dd-0c36cd48e72f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Translator(\n",
              "  (G): Sequential(\n",
              "    (0): Linear(in_features=512, out_features=512, bias=True)\n",
              "    (1): Linear(in_features=512, out_features=512, bias=True)\n",
              "  )\n",
              "  (F): Sequential(\n",
              "    (0): Linear(in_features=512, out_features=512, bias=True)\n",
              "    (1): Linear(in_features=512, out_features=512, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = train_data_fr[0]\n",
        "print(f'input: {x}')\n",
        "xx = torch.tensor(np.array([x[1].numpy()]))\n",
        "sp = (xx == tokenizer.pad_token_id)\n",
        "embs = transformer.encode(xx, sp_mask=sp)\n",
        "en_embs, _ = translate(embs[:,-1:])\n",
        "\n",
        "print('='*26)\n",
        "s = [tokenizer.cls_token_id]\n",
        "for i in range(30):\n",
        "  seq = torch.tensor(np.array([s]))\n",
        "  t_mask = nn.Transformer.generate_square_subsequent_mask(i + 1)\n",
        "  o = real_decoder(en_embs[:,-1:], seq, t_mask)\n",
        "  m = torch.argmax(o, dim=2)\n",
        "  tk = m[0, -1]\n",
        "  s.append(tk)\n",
        "  print(tokenizer.decode(s))\n",
        "  if tk == tokenizer.eos_token_id:\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AbWYfcDduoli",
        "outputId": "f1df57e6-c8bd-48d2-deab-efec8958f974"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input: ('Arsène Lupin est un personnage de fiction français créé par Maurice Leblanc', tensor([  101, 64304, 18105, 23859, 17298, 10176, 10119, 28601, 10104, 20455,\n",
            "        12501, 23673, 10248, 15560, 10281, 22491, 29420,   102,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0]))\n",
            "==========================\n",
            "[CLS] April\n",
            "[CLS] April is\n",
            "[CLS] April is the\n",
            "[CLS] April is the fourth\n",
            "[CLS] April is the fourth month\n",
            "[CLS] April is the fourth month of\n",
            "[CLS] April is the fourth month of the\n",
            "[CLS] April is the fourth month of the year\n",
            "[CLS] April is the fourth month of the year in\n",
            "[CLS] April is the fourth month of the year in the\n",
            "[CLS] April is the fourth month of the year in the Julian\n",
            "[CLS] April is the fourth month of the year in the Julian and\n",
            "[CLS] April is the fourth month of the year in the Julian and Gregor\n",
            "[CLS] April is the fourth month of the year in the Julian and Gregorian\n",
            "[CLS] April is the fourth month of the year in the Julian and Gregorian calendar\n",
            "[CLS] April is the fourth month of the year in the Julian and Gregorian calendars\n",
            "[CLS] April is the fourth month of the year in the Julian and Gregorian calendars,\n",
            "[CLS] April is the fourth month of the year in the Julian and Gregorian calendars, and\n",
            "[CLS] April is the fourth month of the year in the Julian and Gregorian calendars, and comes\n",
            "[CLS] April is the fourth month of the year in the Julian and Gregorian calendars, and comes between\n",
            "[CLS] April is the fourth month of the year in the Julian and Gregorian calendars, and comes between March\n",
            "[CLS] April is the fourth month of the year in the Julian and Gregorian calendars, and comes between March and\n",
            "[CLS] April is the fourth month of the year in the Julian and Gregorian calendars, and comes between March and May\n",
            "[CLS] April is the fourth month of the year in the Julian and Gregorian calendars, and comes between March and May [SEP]\n",
            "[CLS] April is the fourth month of the year in the Julian and Gregorian calendars, and comes between March and May [SEP] [SEP]\n",
            "[CLS] April is the fourth month of the year in the Julian and Gregorian calendars, and comes between March and May [SEP] [SEP] [SEP]\n",
            "[CLS] April is the fourth month of the year in the Julian and Gregorian calendars, and comes between March and May [SEP] [SEP] [SEP] [SEP]\n",
            "[CLS] April is the fourth month of the year in the Julian and Gregorian calendars, and comes between March and May [SEP] [SEP] [SEP] [SEP] [SEP]\n",
            "[CLS] April is the fourth month of the year in the Julian and Gregorian calendars, and comes between March and May [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]\n",
            "[CLS] April is the fourth month of the year in the Julian and Gregorian calendars, and comes between March and May [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]\n"
          ]
        }
      ]
    }
  ]
}