{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "47a3ad42a081420088e7c2b48278edef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0a752201a9b64ebbacd6cc22f146f125",
              "IPY_MODEL_126ea5b3c6b1445caa46f0b9437dd32c",
              "IPY_MODEL_35546bfd51764ac2affa70a7770cba71"
            ],
            "layout": "IPY_MODEL_3d98df730ac749d1844a79ff9694acd5"
          }
        },
        "0a752201a9b64ebbacd6cc22f146f125": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6c078e55b3774d7ca18eb3d472c93618",
            "placeholder": "​",
            "style": "IPY_MODEL_12011e1eab3046099577702d170606e7",
            "value": "100%"
          }
        },
        "126ea5b3c6b1445caa46f0b9437dd32c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_48b420fa3af2414c8e34a33f774b2c54",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_33200c149edd464485f4fb9df25b001b",
            "value": 1
          }
        },
        "35546bfd51764ac2affa70a7770cba71": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_df04fe9f01be4b43a43ae39e862b00b3",
            "placeholder": "​",
            "style": "IPY_MODEL_ce9ec3e5676143729d06dbd2ab7ff261",
            "value": " 1/1 [00:00&lt;00:00,  3.68it/s]"
          }
        },
        "3d98df730ac749d1844a79ff9694acd5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6c078e55b3774d7ca18eb3d472c93618": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "12011e1eab3046099577702d170606e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "48b420fa3af2414c8e34a33f774b2c54": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "33200c149edd464485f4fb9df25b001b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "df04fe9f01be4b43a43ae39e862b00b3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ce9ec3e5676143729d06dbd2ab7ff261": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "eb18838129684adaa43cd1bafb1a2472": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0237a36161d848cdbc925ba3e8777f8c",
              "IPY_MODEL_f793b85981e34ebda3bd03edd57061a9",
              "IPY_MODEL_786e361768124ecd83ea1d928bcf33c9"
            ],
            "layout": "IPY_MODEL_0a89977811b84b279caf6b197e2f3518"
          }
        },
        "0237a36161d848cdbc925ba3e8777f8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1b1de1596fee457181d195cd64e78c20",
            "placeholder": "​",
            "style": "IPY_MODEL_30ea9718707541b88d265c1c171f1da6",
            "value": "100%"
          }
        },
        "f793b85981e34ebda3bd03edd57061a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8e0ebcacaf8742008bb7739efa4052c3",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_764e10aefa4840b3a436e13d10a7c15f",
            "value": 1
          }
        },
        "786e361768124ecd83ea1d928bcf33c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c15c44b9c2914d9eb93f9e5b1b691956",
            "placeholder": "​",
            "style": "IPY_MODEL_415899b7c60e477e891accdfe435be73",
            "value": " 1/1 [00:02&lt;00:00,  2.50s/it]"
          }
        },
        "0a89977811b84b279caf6b197e2f3518": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1b1de1596fee457181d195cd64e78c20": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "30ea9718707541b88d265c1c171f1da6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8e0ebcacaf8742008bb7739efa4052c3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "764e10aefa4840b3a436e13d10a7c15f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c15c44b9c2914d9eb93f9e5b1b691956": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "415899b7c60e477e891accdfe435be73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pinnouse/zeroshot-unsurpervised-mt/blob/main/CSC413_Final_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "QfiQJ3HYviGL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "439345c7-8965-4765-e891-e738c7b9fc68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: apache_beam in /usr/local/lib/python3.9/dist-packages (2.46.0)\n",
            "Requirement already satisfied: mwparserfromhell in /usr/local/lib/python3.9/dist-packages (0.6.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.24.0 in /usr/local/lib/python3.9/dist-packages (from apache_beam) (2.27.1)\n",
            "Requirement already satisfied: hdfs<3.0.0,>=2.1.0 in /usr/local/lib/python3.9/dist-packages (from apache_beam) (2.7.0)\n",
            "Requirement already satisfied: grpcio!=1.48.0,<2,>=1.33.1 in /usr/local/lib/python3.9/dist-packages (from apache_beam) (1.53.0)\n",
            "Requirement already satisfied: pytz>=2018.3 in /usr/local/lib/python3.9/dist-packages (from apache_beam) (2022.7.1)\n",
            "Requirement already satisfied: regex>=2020.6.8 in /usr/local/lib/python3.9/dist-packages (from apache_beam) (2022.10.31)\n",
            "Requirement already satisfied: orjson<4.0 in /usr/local/lib/python3.9/dist-packages (from apache_beam) (3.8.9)\n",
            "Requirement already satisfied: numpy<1.25.0,>=1.14.3 in /usr/local/lib/python3.9/dist-packages (from apache_beam) (1.22.4)\n",
            "Requirement already satisfied: protobuf<4,>3.12.2 in /usr/local/lib/python3.9/dist-packages (from apache_beam) (3.20.3)\n",
            "Requirement already satisfied: zstandard<1,>=0.18.0 in /usr/local/lib/python3.9/dist-packages (from apache_beam) (0.20.0)\n",
            "Requirement already satisfied: proto-plus<2,>=1.7.1 in /usr/local/lib/python3.9/dist-packages (from apache_beam) (1.22.2)\n",
            "Requirement already satisfied: crcmod<2.0,>=1.7 in /usr/local/lib/python3.9/dist-packages (from apache_beam) (1.7)\n",
            "Requirement already satisfied: typing-extensions>=3.7.0 in /usr/local/lib/python3.9/dist-packages (from apache_beam) (4.5.0)\n",
            "Requirement already satisfied: fasteners<1.0,>=0.3 in /usr/local/lib/python3.9/dist-packages (from apache_beam) (0.18)\n",
            "Requirement already satisfied: cloudpickle~=2.2.1 in /usr/local/lib/python3.9/dist-packages (from apache_beam) (2.2.1)\n",
            "Collecting dill<0.3.2,>=0.3.1.1\n",
            "  Using cached dill-0.3.1.1-py3-none-any.whl\n",
            "Requirement already satisfied: pydot<2,>=1.2.0 in /usr/local/lib/python3.9/dist-packages (from apache_beam) (1.4.2)\n",
            "Requirement already satisfied: httplib2<0.22.0,>=0.8 in /usr/local/lib/python3.9/dist-packages (from apache_beam) (0.21.0)\n",
            "Requirement already satisfied: pymongo<4.0.0,>=3.8.0 in /usr/local/lib/python3.9/dist-packages (from apache_beam) (3.13.0)\n",
            "Requirement already satisfied: pyarrow<10.0.0,>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from apache_beam) (9.0.0)\n",
            "Requirement already satisfied: python-dateutil<3,>=2.8.0 in /usr/local/lib/python3.9/dist-packages (from apache_beam) (2.8.2)\n",
            "Requirement already satisfied: fastavro<2,>=0.23.6 in /usr/local/lib/python3.9/dist-packages (from apache_beam) (1.7.3)\n",
            "Requirement already satisfied: objsize<0.7.0,>=0.6.1 in /usr/local/lib/python3.9/dist-packages (from apache_beam) (0.6.1)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.9/dist-packages (from hdfs<3.0.0,>=2.1.0->apache_beam) (1.16.0)\n",
            "Requirement already satisfied: docopt in /usr/local/lib/python3.9/dist-packages (from hdfs<3.0.0,>=2.1.0->apache_beam) (0.6.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.9/dist-packages (from httplib2<0.22.0,>=0.8->apache_beam) (3.0.9)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.24.0->apache_beam) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.24.0->apache_beam) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.24.0->apache_beam) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.24.0->apache_beam) (2022.12.7)\n",
            "Installing collected packages: dill\n",
            "  Attempting uninstall: dill\n",
            "    Found existing installation: dill 0.3.6\n",
            "    Uninstalling dill-0.3.6:\n",
            "      Successfully uninstalled dill-0.3.6\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "multiprocess 0.70.14 requires dill>=0.3.6, but you have dill 0.3.1.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed dill-0.3.1.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (4.27.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.10.7)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.9/dist-packages (2.11.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from datasets) (1.22.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from datasets) (1.4.4)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.9/dist-packages (from datasets) (0.18.0)\n",
            "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (0.3.1.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.9/dist-packages (from datasets) (3.8.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.9/dist-packages (from datasets) (4.65.0)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (0.13.3)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.9/dist-packages (from datasets) (2023.3.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.9/dist-packages (from datasets) (3.2.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from datasets) (23.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (2.27.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.9/dist-packages (from datasets) (0.70.14)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (22.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (1.3.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (1.8.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (2.0.12)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (3.10.7)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
            "Collecting dill<0.3.7,>=0.3.0\n",
            "  Using cached dill-0.3.6-py3-none-any.whl (110 kB)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: dill\n",
            "  Attempting uninstall: dill\n",
            "    Found existing installation: dill 0.3.1.1\n",
            "    Uninstalling dill-0.3.1.1:\n",
            "      Successfully uninstalled dill-0.3.1.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "apache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.6 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed dill-0.3.6\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.9/dist-packages (6.1.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.9/dist-packages (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (4.65.0)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.9/dist-packages (from ftfy) (0.2.6)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-j6ytb21t\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-j6ytb21t\n",
            "  Resolved https://github.com/openai/CLIP.git to commit a9b1bf5920416aaeaec965c25dd9e8f98c864f16\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.9/dist-packages (from clip==1.0) (6.1.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.9/dist-packages (from clip==1.0) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from clip==1.0) (4.65.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (from clip==1.0) (2.0.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.9/dist-packages (from clip==1.0) (0.15.1+cu118)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.9/dist-packages (from ftfy->clip==1.0) (0.2.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch->clip==1.0) (4.5.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch->clip==1.0) (3.1.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch->clip==1.0) (3.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch->clip==1.0) (1.11.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch->clip==1.0) (3.10.7)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch->clip==1.0) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch->clip==1.0) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch->clip==1.0) (16.0.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from torchvision->clip==1.0) (2.27.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision->clip==1.0) (8.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from torchvision->clip==1.0) (1.22.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch->clip==1.0) (2.1.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision->clip==1.0) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision->clip==1.0) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision->clip==1.0) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision->clip==1.0) (2.0.12)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch->clip==1.0) (1.3.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: multilingual-clip in /usr/local/lib/python3.9/dist-packages (1.0.10)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (from multilingual-clip) (4.27.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers->multilingual-clip) (3.10.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers->multilingual-clip) (2.27.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers->multilingual-clip) (1.22.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from transformers->multilingual-clip) (0.13.3)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers->multilingual-clip) (0.13.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers->multilingual-clip) (2022.10.31)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers->multilingual-clip) (4.65.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers->multilingual-clip) (23.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers->multilingual-clip) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers->multilingual-clip) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers->multilingual-clip) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers->multilingual-clip) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers->multilingual-clip) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers->multilingual-clip) (1.26.15)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.9/dist-packages (2.2.2)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (2.0.0+cu118)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (4.65.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (0.1.97)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (4.27.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (3.8.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (1.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (1.22.4)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (0.15.1+cu118)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (1.10.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (0.13.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.27.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (23.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.10.7)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch>=1.6.0->sentence-transformers) (3.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch>=1.6.0->sentence-transformers) (1.11.1)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch>=1.6.0->sentence-transformers) (2.0.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch>=1.6.0->sentence-transformers) (3.1.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence-transformers) (16.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence-transformers) (3.25.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2022.10.31)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.13.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from nltk->sentence-transformers) (1.1.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from nltk->sentence-transformers) (8.1.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->sentence-transformers) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision->sentence-transformers) (8.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch>=1.6.0->sentence-transformers) (2.1.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.26.15)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch>=1.6.0->sentence-transformers) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install apache_beam mwparserfromhell\n",
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install ftfy regex tqdm\n",
        "!pip install git+https://github.com/openai/CLIP.git\n",
        "\n",
        "# multilingual CLIP pretrained\n",
        "# https://github.com/FreddeFrallan/Multilingual-CLIP\n",
        "!pip install multilingual-clip\n",
        "!pip install -U sentence-transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Processing"
      ],
      "metadata": {
        "id": "sWOtaVj6Zlf1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"wikipedia\", \"20220301.simple\")\n",
        "dataset_fr = load_dataset(\"wikipedia\", \"20220301.fr\")"
      ],
      "metadata": {
        "id": "ohhofFRj0sMy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137,
          "referenced_widgets": [
            "47a3ad42a081420088e7c2b48278edef",
            "0a752201a9b64ebbacd6cc22f146f125",
            "126ea5b3c6b1445caa46f0b9437dd32c",
            "35546bfd51764ac2affa70a7770cba71",
            "3d98df730ac749d1844a79ff9694acd5",
            "6c078e55b3774d7ca18eb3d472c93618",
            "12011e1eab3046099577702d170606e7",
            "48b420fa3af2414c8e34a33f774b2c54",
            "33200c149edd464485f4fb9df25b001b",
            "df04fe9f01be4b43a43ae39e862b00b3",
            "ce9ec3e5676143729d06dbd2ab7ff261",
            "eb18838129684adaa43cd1bafb1a2472",
            "0237a36161d848cdbc925ba3e8777f8c",
            "f793b85981e34ebda3bd03edd57061a9",
            "786e361768124ecd83ea1d928bcf33c9",
            "0a89977811b84b279caf6b197e2f3518",
            "1b1de1596fee457181d195cd64e78c20",
            "30ea9718707541b88d265c1c171f1da6",
            "8e0ebcacaf8742008bb7739efa4052c3",
            "764e10aefa4840b3a436e13d10a7c15f",
            "c15c44b9c2914d9eb93f9e5b1b691956",
            "415899b7c60e477e891accdfe435be73"
          ]
        },
        "outputId": "7f28b9b3-3d4c-40b3-cff7-e29b31246d38"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.builder:Found cached dataset wikipedia (/root/.cache/huggingface/datasets/wikipedia/20220301.simple/2.0.0/aa542ed919df55cc5d3347f42dd4521d05ca68751f50dbc32bae2a7f1e167559)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "47a3ad42a081420088e7c2b48278edef"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.builder:Found cached dataset wikipedia (/root/.cache/huggingface/datasets/wikipedia/20220301.fr/2.0.0/aa542ed919df55cc5d3347f42dd4521d05ca68751f50dbc32bae2a7f1e167559)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "eb18838129684adaa43cd1bafb1a2472"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')"
      ],
      "metadata": {
        "id": "j49MEp7YaY8c"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = map(lambda x: x['text'].replace(\"\\n\", ' ').split(\". \"), dataset['train'])\n",
        "text_long = []\n",
        "for t in text:\n",
        "  for s in t:\n",
        "    text_long.append(s) "
      ],
      "metadata": {
        "id": "rqX89Co51UGD"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_fr = map(lambda x: x['text'].replace(\"\\n\", ' ').split(\". \"), dataset_fr['train'])\n",
        "for t in text_fr:\n",
        "  print(t)\n",
        "  break"
      ],
      "metadata": {
        "id": "e2J--DFUD4MK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8231ebb0-6d37-4d29-a1d4-5b29f11ae938"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Paul Jules Antoine Meillet, né le  à Moulins (Allier) et mort le  à Châteaumeillant (Cher), est le principal linguiste français des premières décennies du ', 'Il est aussi philologue', \" Biographie  D'origine bourbonnaise, fils d'un notaire de Châteaumeillant (Cher), Antoine Meillet fait ses études secondaires au lycée de Moulins\", \" Étudiant à la faculté des lettres de Paris à partir de 1885 où il suit notamment les cours de Louis Havet, il assiste également à ceux de Michel Bréal au Collège de France et de Ferdinand de Saussure à l'École pratique des hautes études\", \" En 1889, il est major de l'agrégation de grammaire\", \" Il assure à la suite de Saussure le cours de grammaire comparée, qu'il complète à partir de 1894 par une conférence sur les langues persanes\", \" En 1897, il soutient sa thèse pour le doctorat ès lettres (Recherches sur l'emploi du génitif-accusatif en vieux-slave)\", \"En 1905, il occupe la chaire de grammaire comparée au Collège de France, où il consacre ses cours à l'histoire et à la structure des langues indo-européennes\", \"Il succéda au linguiste Auguste Carrière à la tête de la chaire d'arménien à l'École des langues orientales\", \" Secrétaire de la Société de linguistique de Paris, il est élu à l'Académie des inscriptions et belles-lettres en 1924\", \"Il préside également l'Institut d'Études Slaves de 1921 à sa mort\", ' Il a formé toute une génération de linguistes français, parmi lesquels Émile Benveniste, Marcel Cohen, Georges Dumézil, André Martinet, Aurélien Sauvageot, Lucien Tesnière, Joseph Vendryes, ainsi que le japonisant Charles Haguenauer', \"Antoine Meillet devait diriger la thèse de Jean Paulhan sur la sémantique du proverbe et c'est lui qui découvrit Gustave Guillaume\", ' Il a influencé aussi un certain nombre de linguistes étrangers', 'Il a également été le premier à identifier le phénomène de la grammaticalisation', ' Selon le linguiste allemand Walter Porzig, Meillet est un « grand précurseur »', \"Il montre, par exemple, que, dans les dialectes indo-européens, les groupes indo-européens sont le résultat historique d'une variation diatopique\", ' L’acte de naissance de la sociolinguistique est signé par Antoine Meillet fondateur de la sociolinguistique qui s’est opposé au Cours de linguistique générale de Ferdinand de Saussure dès son apparition en 1916 en le critiquant sur plusieurs plans', \" Études arméniennes   1890 : une mission de trois mois dans le Caucase lui permet d'apprendre l'arménien moderne\", \" 1902 : il obtient la chaire d'arménien de l'École des langues orientales\", \" 1903 : nouvelle mission en Arménie russe, il publie son Esquisse d'une grammaire comparée de l'arménien classique, qui demeure une référence en linguistique arménienne et indo-européenne jusqu'à ce jour\", \"L'un de ses étudiants, Hratchia Adjarian, devient le fondateur de la dialectologie arménienne\", \"C'est également sous les encouragements de Meillet qu'Émile Benveniste étudie la langue arménienne\", ' 1919 : il est cofondateur de la Société des études arméniennes avec Victor Bérard, Charles Diehl, André-Ferdinand Hérold, H', 'Lacroix, Frédéric Macler, Gabriel Millet, Gustave Schlumberger', ' 1920 : le , il crée la Revue des études arméniennes avec Frédéric Macler', ' Études homériques  À la Sorbonne, Meillet supervise le travail de Milman Parry', \"Meillet offre à son étudiant l'opinion, nouvelle à cette époque, que la structure formulaïque de l'Iliade serait une conséquence directe de sa transmission orale\", \"Ainsi, il le dirige vers l'étude de l'oralité dans son cadre natif et lui suggère d'observer les mécanismes d'une tradition orale vivante à côté du texte classique (l'Iliade) qui est censé résulter d'une telle tradition\", 'En conséquence, Meillet présente Parry à Matija Murko, savant originaire de Slovénie qui avait longuement écrit sur la tradition héroïque épique dans les Balkans, surtout en Bosnie-Herzégovine', \"Par leurs recherches, dont les résultats sont à présent hébergés par l'université de Harvard, Parry et son élève, Albert Lord, ont profondément renouvelé les études homériques\", \" Principaux ouvrages   Études sur l'étymologie et le vocabulaire du vieux slave\", 'Paris, Bouillon, 1902-05', \" Esquisse d'une grammaire comparée de l'arménien classique, 1903\", \" Introduction à l'étude comparative des langues indo-européennes, 1903 ( éd.), Hachette, Paris, 1912 ( éd.)\", ' Les dialectes indo-européens, 1908', \" Aperçu d'une histoire de la langue grecque, 1913\", ' Altarmenisches Elementarbuch, 1913', \"Heidelberg (en français : Manuel élémentaire d'Arménien classique, traduction de Gabriel Képéklian, Limoges, Lambert-Lucas, 2017 )  Caractères généraux des langues germaniques, 1917, rev\", 'edn', '1949', ' Linguistique historique et linguistique générale, 1921 (le tome II est paru en 1936 ; les deux tomes ont été réunis chez Lambert-Lucas, Limoges, 2015)', ' Les origines indo-européennes des mètres grecs, 1923', ' Traité de grammaire comparée des langues classiques, 1924 (avec Joseph Vendryés)', ' La méthode comparative en linguistique historique, 1925, Oslo, Instituttet for Sammenlignende Kulturforskning (réimpr', 'Paris, Champion, 1954)', ' ', ' Dictionnaire étymologique de la langue latine, 1932 (en collab', 'Avec Alfred Ernout (1879-1973), éd', 'augmentée, par Jacques André (1910-1994), Paris : Klincksieck, 2001,    Meillet en Arménie, 1891, 1903, Journaux et lettres publiés par Francis Gandon, Limoges, Lambert-Lucas, 2014, ', ' Notes et références  Voir aussi  Bibliographie   Marc Décimo, Sciences et pataphysique, t', '2 : Comment la linguistique vint à Paris ?, De Michel Bréal à Ferdinand de Saussure, Dijon, Les Presses du réel, coll', 'Les Hétéroclites, 2014 ', \" Articles connexes   Franz Bopp  Johann Kaspar Zeuss  Liens externes         Commandeur de la Légion d'honneur Académie des inscriptions et belles-lettres Agrégé de grammaire Linguiste français Philologue français Slaviste Personnalité liée à la langue kurde Institut national des langues et civilisations orientales Arménologue français Indo-européaniste Étudiant de l'université de Paris Naissance en novembre 1866 Naissance à Moulins (Allier) Décès en septembre 1936 Décès à 69 ans Décès dans le Cher Personnalité inhumée à Moulins\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchtext\n",
        "import clip\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
        "\n",
        "text_model = SentenceTransformer('sentence-transformers/clip-ViT-B-32-multilingual-v1')\n",
        "\n",
        "context_length = 64\n",
        "\n",
        "#glove = torchtext.vocab.GloVe(name=\"6B\", dim=50)\n",
        "#ft = torchtext.vocab.FastText(language=\"simple\")"
      ],
      "metadata": {
        "id": "lQAoFq3n3F2R"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenize/Embbed English data"
      ],
      "metadata": {
        "id": "Eluf1BFlhNVp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_eng = []\n",
        "for (i, t) in enumerate(text_long):\n",
        "  if i > 500:\n",
        "    break\n",
        "  tokenized = tokenizer(t, padding='max_length', max_length=64, return_tensors='pt').input_ids #took 18 mins to run\n",
        "  if len(tokenized) <= 64:\n",
        "    clip_emb = text_model.encode(t)\n",
        "    train_data_eng.append((t,clip_emb,tokenized[0]))"
      ],
      "metadata": {
        "id": "8YbUCvXbguuv"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenize/Embbed French data"
      ],
      "metadata": {
        "id": "hEjJsAnhhUBR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_fr = []\n",
        "\n",
        "for (i, t) in enumerate(text_fr):\n",
        "  if i > 100:\n",
        "    break\n",
        "  sentence = t[0]\n",
        "  if len(sentence.split(' ')) > 60:\n",
        "    continue\n",
        "  tokenized = tokenizer(sentence, padding='max_length', max_length=64, return_tensors='pt')['input_ids']\n",
        "  # tokenized = tokenized.to(device)\n",
        "  train_data_fr.append((sentence, tokenized[0]))"
      ],
      "metadata": {
        "id": "8pt4ikdkG_cm"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GAN+Autoencoder Idea\n",
        "![Diagram](https://media.discordapp.net/attachments/1085342382679138385/1090810686793318491/image.png?width=1440&height=592)\n",
        "Orange represents the zeroshot translation task"
      ],
      "metadata": {
        "id": "eB0NhhxkSKnG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Models\n",
        "\n",
        "We will create 4 models:\n",
        "1. Monolingual generator (expert at i.e. FR, DE) - transformer encoder+decoder\n",
        "2. CLIP Embedding Decoder - transformer decoders\n",
        "3. Discriminator - linear neural network (subject to change)\n",
        "4. Translator - linear model from embedding space to embedding space (thought of as a transform function from FR -> EN)\n",
        "\n",
        "We refer to 2 agents that are learning to understand their respective languages, both represented by transformers.\n",
        "`Base expert` is the agent that 'understands' English (from CLIP).\n",
        "`Other expert` is agent that is an expert of the other language."
      ],
      "metadata": {
        "id": "JOIeUeVNZGQh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "from torch import Tensor\n",
        "import math\n",
        "\n",
        "# https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
        "class PositionalEncoding(nn.Module):\n",
        "  def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
        "    super().__init__()\n",
        "    self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "    position = torch.arange(max_len).unsqueeze(1)\n",
        "    div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "    pe = torch.zeros(max_len, 1, d_model)\n",
        "    pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
        "    pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
        "    self.register_buffer('pe', pe)\n",
        "\n",
        "  def forward(self, x: Tensor) -> Tensor:\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
        "    \"\"\"\n",
        "    x = x + self.pe[:x.size(0)]\n",
        "    return self.dropout(x)\n",
        "\n",
        "# https://jamesmccaffrey.wordpress.com/2022/09/09/simplest-transformer-seq-to-seq-example/\n",
        "class Transformer(nn.Module): # same language/monolingual\n",
        "  def __init__(self, ntoken: int, d_model: int = 512, nhead: int = 4, d_hid: int = 512,\n",
        "               nlayers: int = 4, dropout: float = 0.5):\n",
        "    super().__init__()\n",
        "\n",
        "    self.model_type = 'Transformer'\n",
        "\n",
        "    self.embed = nn.Embedding(ntoken, d_model)\n",
        "\n",
        "    self.pos_enc = PositionalEncoding(d_model, dropout)\n",
        "    encoder_layers = nn.TransformerEncoderLayer(d_model, nhead, d_hid, dropout,\n",
        "                                                batch_first=True)\n",
        "    self.encoder = nn.TransformerEncoder(encoder_layers, nlayers)\n",
        "\n",
        "    decoder_layers = nn.TransformerDecoderLayer(d_model, nhead, d_hid, dropout,\n",
        "                                                batch_first=True)\n",
        "    self.decoder = nn.TransformerDecoder(decoder_layers, nlayers)\n",
        "\n",
        "    self.d_model = d_model\n",
        "\n",
        "    self.dense = nn.Linear(d_model, ntoken)\n",
        "\n",
        "    self.init_weights()\n",
        "\n",
        "  def init_weights(self) -> None:\n",
        "    initrange = 0.1\n",
        "    self.dense.weight.data.uniform_(-initrange, initrange)\n",
        "    self.dense.bias.data.zero_()\n",
        "\n",
        "  def forward(self, src, tgt, tgt_mask, tp_mask=None, sp_mask=None):\n",
        "    s = self.embed(src)\n",
        "    t = self.embed(tgt)\n",
        "\n",
        "    s = self.pos_enc(s)\n",
        "    t = self.pos_enc(t)\n",
        "\n",
        "    e = self.encoder(s, src_key_padding_mask=sp_mask)\n",
        "    d = self.decoder(t, e, tgt_mask=tgt_mask, tgt_key_padding_mask=tp_mask, memory_key_padding_mask=sp_mask)\n",
        "    z = self.dense(d)\n",
        "    # z = nn.functional.softmax(z, dim=2)\n",
        "    return z, e\n",
        "\n",
        "class Translator(nn.Module):\n",
        "  def __init__(self, i_embed_size = 512, o_embed_size = 512, nlayers = 1, hidden = 1024):\n",
        "    super().__init__()\n",
        "    \n",
        "    # based off https://arxiv.org/abs/1809.03633\n",
        "    self.G = nn.Sequential(nn.Linear(i_embed_size, hidden))\n",
        "    self.F = nn.Sequential(nn.Linear(o_embed_size, hidden))\n",
        "    for i in range(nlayers-1):\n",
        "      self.G.append(nn.Linear(hidden, hidden))\n",
        "      self.F.append(nn.Linear(hidden, hidden))\n",
        "    self.G.append(nn.Linear(hidden, o_embed_size))\n",
        "    self.F.append(nn.Linear(hidden, i_embed_size))\n",
        "\n",
        "  def forward(self, i_emb):\n",
        "    o = self.G(i_emb)\n",
        "    i = self.F(o)\n",
        "    return o, i\n",
        "  \n",
        "class Decoder(nn.Module):\n",
        "  def __init__(self, ntoken: int, d_model: int = 512, nhead: int = 4, d_hid: int = 512,\n",
        "               dropout: float = 0.5, nlayers: int = 4):\n",
        "    super().__init__()\n",
        "    self.embed = nn.Embedding(ntoken, d_model)\n",
        "    self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
        "\n",
        "    decoder_layers = nn.TransformerDecoderLayer(d_model, nhead, d_hid, dropout)\n",
        "    self.decoder = nn.TransformerDecoder(decoder_layers, nlayers)\n",
        "  \n",
        "  def forward(self, enc, tgt, tgt_mask):\n",
        "    t = self.embed(tgt)\n",
        "    t = self.pos_enc(t)\n",
        "\n",
        "    d = self.decoder(tgt, enc, tgt_mask=tgt_mask)\n",
        "    return d\n",
        "    \n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(500, 100),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(100, 1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)\n",
        "        out = self.model(x)\n",
        "        return out.view(x.size(0))"
      ],
      "metadata": {
        "id": "IIZjQ5GwAftg"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train Loop\n",
        "\n",
        "PyTorch training for GAN:\n",
        "[PyTorch blog](https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html)"
      ],
      "metadata": {
        "id": "VhYGAHMKZJhV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import optim\n",
        "# https://huggingface.co/docs/transformers/main_classes/optimizer_schedules#transformers.Adafactor\n",
        "from transformers.optimization import Adafactor\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "transformer = Transformer(tokenizer.vocab_size) # temp vocab size\n",
        "translate = Translator(hidden=512)\n",
        "\n",
        "context_length = 64\n",
        "\n",
        "# TODO: add device specific code\n",
        "\n",
        "def pad(tokens, context_length):\n",
        "  l = list(tokens)\n",
        "  for _ in range(context_length - len(tokens)):\n",
        "    l.append(tokenizer.pad_token_id)\n",
        "  return np.array(l)\n",
        "\n",
        "def train(real_decoder, transformer, discriminator, translate, # our four models\n",
        "          real_train, other_train, real_valid = None, other_valid = None,\n",
        "          epochs = 10, batch_size = 256):\n",
        "  batch_data = []\n",
        "\n",
        "  data_loader = [(real_train, other_train)]\n",
        "\n",
        "  criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
        "  mse = nn.MSELoss()\n",
        "\n",
        "  # r_optim = optim.RAdam(real_decoder.parameters())\n",
        "  # g_optim = optim.RAdam(generator.parameters())\n",
        "  # r_optim = Adafactor(real_decoder.parameters())\n",
        "  g_optim = Adafactor(transformer.parameters())\n",
        "  t_optim = Adafactor(translate.parameters())\n",
        "  # d_optim = optim.Adam(discriminator.parameters())\n",
        "\n",
        "  r_iterations = len(real_train) // batch_size\n",
        "  o_iterations = len(other_train) // batch_size\n",
        "  # r_iterations = real_train.shape[0] // batch_size\n",
        "  # o_iterations = other_train.shape[0] // batch_size\n",
        "\n",
        "  n = min(r_iterations, o_iterations)\n",
        "\n",
        "  losses = []\n",
        "\n",
        "  for e in range(epochs):\n",
        "    epoch_loss = 0\n",
        "    print(f'Epoch {e+1}:')\n",
        "    for i, (r_x, o_x) in enumerate(data_loader):\n",
        "      # r_x: (english sentence: str, CLIP embeddings: float[512], tokens: one-hots[num tokens])\n",
        "      # o_x:\n",
        "      # if r_x.shape[0] % batch_size != 0: # not full batch\n",
        "      #   break\n",
        "      if (i + 1) % 100 == 0:\n",
        "        print(f'Iteration {i+1} of {n}')\n",
        "\n",
        "      # learn decoder\n",
        "      # r_optim.zero_grad()\n",
        "      # ...\n",
        "\n",
        "      # \"other\" generator self learning\n",
        "      # https://jamesmccaffrey.wordpress.com/2022/09/09/simplest-transformer-seq-to-seq-example/\n",
        "      xx = list(map(lambda x: x[1].numpy(), o_x))\n",
        "      xx = torch.tensor(np.array(xx))\n",
        "      \n",
        "      src = xx\n",
        "      tgt = src\n",
        "      tgt_in = tgt[:,:-1]\n",
        "      tgt_expect = tgt[:,1:]\n",
        "      t_mask = nn.Transformer.generate_square_subsequent_mask(context_length - 1)\n",
        "      # https://pytorch.org/tutorials/beginner/translation_transformer.html#seq2seq-network-using-transformer\n",
        "      tgt_attn_mask = (tgt_in == tokenizer.pad_token_id)\n",
        "      attn_mask = (src == tokenizer.pad_token_id)\n",
        "\n",
        "      output, other_embeddings = transformer(src, tgt_in, tgt_mask=t_mask, tp_mask=tgt_attn_mask, sp_mask=attn_mask) # [bs,seq,vocab]\n",
        "\n",
        "      # get preds shape to conform to tgt_expect\n",
        "      output = output.permute(0,2,1)  # now [bs, vocab, seq]\n",
        "\n",
        "      loss = criterion(output, tgt_expect)\n",
        "      epoch_loss += loss.item()\n",
        "\n",
        "      g_optim.zero_grad()\n",
        "      loss.backward()\n",
        "      g_optim.step()\n",
        "    print(f'\\ttrain loss: {epoch_loss}')\n",
        "    losses.append(epoch_loss)\n",
        "  # plt.subplot(0)\n",
        "  plt.xticks(np.arange(epochs))\n",
        "  plt.plot(epoch_loss)\n",
        "  plt.show()\n",
        "\n",
        "print(train_data_fr[0])\n",
        "train(None, transformer, None, translate, [], [train_data_fr[0]] * 16, epochs=100)"
      ],
      "metadata": {
        "id": "XFinB2ZTMh7B",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e57c8b62-0128-4297-bceb-fc3946cceb15"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(\"L’algèbre linéaire est la branche des mathématiques qui s'intéresse aux espaces vectoriels et aux transformations linéaires, formalisation générale des théories des systèmes d'équations linéaires\", tensor([  101,   149,   100, 10164, 10240, 13340, 13724, 11614, 11246, 16556,\n",
            "        10176, 10109, 58760, 10139, 71777, 10355,   187,   112, 26391, 20017,\n",
            "        12818, 10754, 84355, 46514, 19428, 10107, 10131, 10754, 44510, 10107,\n",
            "        11614, 11246, 29194,   117, 23129, 20312, 28274, 10139, 38914, 10107,\n",
            "        10139, 48273,   172,   112,   263, 32973, 15024, 11614, 11246, 29194,\n",
            "          102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0]))\n",
            "Epoch 1:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\ttrain loss: 12.529891014099121\n",
            "Epoch 2:\n",
            "\ttrain loss: 10.283712387084961\n",
            "Epoch 3:\n",
            "\ttrain loss: 8.806689262390137\n",
            "Epoch 4:\n",
            "\ttrain loss: 8.091115951538086\n",
            "Epoch 5:\n",
            "\ttrain loss: 7.325162887573242\n",
            "Epoch 6:\n",
            "\ttrain loss: 6.720075607299805\n",
            "Epoch 7:\n",
            "\ttrain loss: 6.2231764793396\n",
            "Epoch 8:\n",
            "\ttrain loss: 5.82277774810791\n",
            "Epoch 9:\n",
            "\ttrain loss: 5.524688720703125\n",
            "Epoch 10:\n",
            "\ttrain loss: 5.360691547393799\n",
            "Epoch 11:\n",
            "\ttrain loss: 5.171264171600342\n",
            "Epoch 12:\n",
            "\ttrain loss: 4.5983662605285645\n",
            "Epoch 13:\n",
            "\ttrain loss: 4.376806735992432\n",
            "Epoch 14:\n",
            "\ttrain loss: 4.357534408569336\n",
            "Epoch 15:\n",
            "\ttrain loss: 3.7334892749786377\n",
            "Epoch 16:\n",
            "\ttrain loss: 3.593440055847168\n",
            "Epoch 17:\n",
            "\ttrain loss: 3.371273994445801\n",
            "Epoch 18:\n",
            "\ttrain loss: 3.5296051502227783\n",
            "Epoch 19:\n",
            "\ttrain loss: 3.333890676498413\n",
            "Epoch 20:\n",
            "\ttrain loss: 3.0240259170532227\n",
            "Epoch 21:\n",
            "\ttrain loss: 2.374063491821289\n",
            "Epoch 22:\n",
            "\ttrain loss: 2.288400888442993\n",
            "Epoch 23:\n",
            "\ttrain loss: 2.0318851470947266\n",
            "Epoch 24:\n",
            "\ttrain loss: 1.6393728256225586\n",
            "Epoch 25:\n",
            "\ttrain loss: 1.4813652038574219\n",
            "Epoch 26:\n",
            "\ttrain loss: 1.3222395181655884\n",
            "Epoch 27:\n",
            "\ttrain loss: 1.0684694051742554\n",
            "Epoch 28:\n",
            "\ttrain loss: 0.7814606428146362\n",
            "Epoch 29:\n",
            "\ttrain loss: 0.7044104933738708\n",
            "Epoch 30:\n",
            "\ttrain loss: 0.5662389993667603\n",
            "Epoch 31:\n",
            "\ttrain loss: 0.5351465344429016\n",
            "Epoch 32:\n",
            "\ttrain loss: 0.41824331879615784\n",
            "Epoch 33:\n",
            "\ttrain loss: 0.45089682936668396\n",
            "Epoch 34:\n",
            "\ttrain loss: 0.4848020076751709\n",
            "Epoch 35:\n",
            "\ttrain loss: 0.5487539768218994\n",
            "Epoch 36:\n",
            "\ttrain loss: 0.4821258783340454\n",
            "Epoch 37:\n",
            "\ttrain loss: 0.38260409235954285\n",
            "Epoch 38:\n",
            "\ttrain loss: 0.33316993713378906\n",
            "Epoch 39:\n",
            "\ttrain loss: 0.35109832882881165\n",
            "Epoch 40:\n",
            "\ttrain loss: 0.3100733458995819\n",
            "Epoch 41:\n",
            "\ttrain loss: 0.28351128101348877\n",
            "Epoch 42:\n",
            "\ttrain loss: 0.2941026985645294\n",
            "Epoch 43:\n",
            "\ttrain loss: 0.2668295204639435\n",
            "Epoch 44:\n",
            "\ttrain loss: 0.24796751141548157\n",
            "Epoch 45:\n",
            "\ttrain loss: 0.2266884297132492\n",
            "Epoch 46:\n",
            "\ttrain loss: 0.2235960066318512\n",
            "Epoch 47:\n",
            "\ttrain loss: 0.2269614040851593\n",
            "Epoch 48:\n",
            "\ttrain loss: 0.2737697660923004\n",
            "Epoch 49:\n",
            "\ttrain loss: 0.23832382261753082\n",
            "Epoch 50:\n",
            "\ttrain loss: 0.23189739882946014\n",
            "Epoch 51:\n",
            "\ttrain loss: 0.23343876004219055\n",
            "Epoch 52:\n",
            "\ttrain loss: 0.238749697804451\n",
            "Epoch 53:\n",
            "\ttrain loss: 0.19737590849399567\n",
            "Epoch 54:\n",
            "\ttrain loss: 0.17787183821201324\n",
            "Epoch 55:\n",
            "\ttrain loss: 0.14389944076538086\n",
            "Epoch 56:\n",
            "\ttrain loss: 0.1443977653980255\n",
            "Epoch 57:\n",
            "\ttrain loss: 0.14871150255203247\n",
            "Epoch 58:\n",
            "\ttrain loss: 0.15892334282398224\n",
            "Epoch 59:\n",
            "\ttrain loss: 0.15774114429950714\n",
            "Epoch 60:\n",
            "\ttrain loss: 0.15409035980701447\n",
            "Epoch 61:\n",
            "\ttrain loss: 0.16945230960845947\n",
            "Epoch 62:\n",
            "\ttrain loss: 0.1575341820716858\n",
            "Epoch 63:\n",
            "\ttrain loss: 0.1504857838153839\n",
            "Epoch 64:\n",
            "\ttrain loss: 0.12435810267925262\n",
            "Epoch 65:\n",
            "\ttrain loss: 0.10802367329597473\n",
            "Epoch 66:\n",
            "\ttrain loss: 0.09321389347314835\n",
            "Epoch 67:\n",
            "\ttrain loss: 0.10477212071418762\n",
            "Epoch 68:\n",
            "\ttrain loss: 0.10971405357122421\n",
            "Epoch 69:\n",
            "\ttrain loss: 0.097829669713974\n",
            "Epoch 70:\n",
            "\ttrain loss: 0.08930733799934387\n",
            "Epoch 71:\n",
            "\ttrain loss: 0.07338883727788925\n",
            "Epoch 72:\n",
            "\ttrain loss: 0.07822366058826447\n",
            "Epoch 73:\n",
            "\ttrain loss: 0.07120350003242493\n",
            "Epoch 74:\n",
            "\ttrain loss: 0.0677805244922638\n",
            "Epoch 75:\n",
            "\ttrain loss: 0.06480139493942261\n",
            "Epoch 76:\n",
            "\ttrain loss: 0.04707761108875275\n",
            "Epoch 77:\n",
            "\ttrain loss: 0.061578795313835144\n",
            "Epoch 78:\n",
            "\ttrain loss: 0.040602218359708786\n",
            "Epoch 79:\n",
            "\ttrain loss: 0.04933592677116394\n",
            "Epoch 80:\n",
            "\ttrain loss: 0.05204465240240097\n",
            "Epoch 81:\n",
            "\ttrain loss: 0.046078816056251526\n",
            "Epoch 82:\n",
            "\ttrain loss: 0.03548094630241394\n",
            "Epoch 83:\n",
            "\ttrain loss: 0.03468431532382965\n",
            "Epoch 84:\n",
            "\ttrain loss: 0.027918018400669098\n",
            "Epoch 85:\n",
            "\ttrain loss: 0.02847907692193985\n",
            "Epoch 86:\n",
            "\ttrain loss: 0.033538080751895905\n",
            "Epoch 87:\n",
            "\ttrain loss: 0.023704340681433678\n",
            "Epoch 88:\n",
            "\ttrain loss: 0.022690149024128914\n",
            "Epoch 89:\n",
            "\ttrain loss: 0.025601675733923912\n",
            "Epoch 90:\n",
            "\ttrain loss: 0.028994107618927956\n",
            "Epoch 91:\n",
            "\ttrain loss: 0.027927400544285774\n",
            "Epoch 92:\n",
            "\ttrain loss: 0.02375507541000843\n",
            "Epoch 93:\n",
            "\ttrain loss: 0.02053702436387539\n",
            "Epoch 94:\n",
            "\ttrain loss: 0.022978300228714943\n",
            "Epoch 95:\n",
            "\ttrain loss: 0.01891058124601841\n",
            "Epoch 96:\n",
            "\ttrain loss: 0.026325363665819168\n",
            "Epoch 97:\n",
            "\ttrain loss: 0.018365520983934402\n",
            "Epoch 98:\n",
            "\ttrain loss: 0.017460599541664124\n",
            "Epoch 99:\n",
            "\ttrain loss: 0.023683954030275345\n",
            "Epoch 100:\n",
            "\ttrain loss: 0.022538885474205017\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-f57117a3f136>\u001b[0m in \u001b[0;36m<cell line: 92>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_fr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranslate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtrain_data_fr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-39-f57117a3f136>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(real_decoder, transformer, discriminator, translate, real_train, other_train, real_valid, other_valid, epochs, batch_size)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'\\ttrain loss: {epoch_loss}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m   \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m   \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxticks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m   \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36msubplot\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;31m# First, search for an existing subplot with a matching spec.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m     \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSubplotSpec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_from_subplot_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0max\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/matplotlib/gridspec.py\u001b[0m in \u001b[0;36m_from_subplot_args\u001b[0;34m(figure, args)\u001b[0m\n\u001b[1;32m    583\u001b[0m             \u001b[0mrows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 585\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnargs_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"subplot\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtakes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"1 or 3\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgiven\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0mgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSpec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_gridspec_exists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: subplot() takes 1 or 3 positional arguments but 2 were given"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t = pad(train_data_fr[0][1], context_length)\n",
        "src = torch.tensor(np.array([t]))\n",
        "print(src.shape)\n",
        "print(t)\n",
        "# should predict 5,6,7,8,7,6,5\n",
        "\n",
        "with torch.no_grad():\n",
        "  s = list(t[0:1])\n",
        "  for i in range(30):\n",
        "    tgt_in = torch.tensor([s])\n",
        "    t_mask = nn.Transformer.generate_square_subsequent_mask(len(s))\n",
        "    tgt_attn_mask = (tgt_in == tokenizer.pad_token_id)\n",
        "    attn_mask = src == tokenizer.pad_token_id\n",
        "    # print(tgt_in)\n",
        "    preds, emb = transformer(src, tgt_in, tgt_mask=t_mask, tp_mask=tgt_attn_mask, sp_mask=attn_mask)\n",
        "    m = torch.argmax(preds, dim=2)\n",
        "    print(m)\n",
        "    tk = m[0][-1]\n",
        "    if tk == tokenizer.eos_token_id:\n",
        "      break\n",
        "    word = tokenizer.decode(tk)\n",
        "    print(word)\n",
        "    s.append(tk)\n",
        "  print(tokenizer.decode(s))\n",
        "# result is 12 logits where largest is at the\n",
        "# predicted token"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2e4P31Dg1msK",
        "outputId": "54153c0f-1352-4e0f-d62d-6bd1aac83a46"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 64])\n",
            "[  101   149   100 10164 10240 13340 13724 11614 11246 16556 10176 10109\n",
            " 58760 10139 71777 10355   187   112 26391 20017 12818 10754 84355 46514\n",
            " 19428 10107 10131 10754 44510 10107 11614 11246 29194   117 23129 20312\n",
            " 28274 10139 38914 10107 10139 48273   172   112   263 32973 15024 11614\n",
            " 11246 29194   102     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0]\n",
            "tensor([[149]])\n",
            "L\n",
            "tensor([[149, 100]])\n",
            "[ U N K ]\n",
            "tensor([[  149,   100, 10164]])\n",
            "a l\n",
            "tensor([[  149,   100, 10164, 10240]])\n",
            "# # g\n",
            "tensor([[  149,   100, 10164, 10240, 13340]])\n",
            "# # è\n",
            "tensor([[  149,   100, 10164, 10240, 13340, 13724]])\n",
            "# # b r e\n",
            "tensor([[  149,   100, 10164, 10240, 13340, 13724, 11614]])\n",
            "l i\n",
            "tensor([[  149,   100, 10164, 10240, 13340, 13724, 11614, 11246]])\n",
            "# # n é\n",
            "tensor([[  149,   100, 10164, 10240, 13340, 13724, 11614, 11246, 16556]])\n",
            "# # a i r e\n",
            "tensor([[  149,   100, 10164, 10240, 13340, 13724, 11614, 11246, 16556, 10176]])\n",
            "e s t\n",
            "tensor([[  149,   100, 10164, 10240, 13340, 13724, 11614, 11246, 16556, 10176,\n",
            "         10109]])\n",
            "l a\n",
            "tensor([[  149,   100, 10164, 10240, 13340, 13724, 11614, 11246, 16556, 10176,\n",
            "         10109, 58760]])\n",
            "b r a n c h e\n",
            "tensor([[  149,   100, 10164, 10240, 13340, 13724, 11614, 11246, 16556, 10176,\n",
            "         10109, 58760, 10139]])\n",
            "d e s\n",
            "tensor([[  149,   100, 10164, 10240, 13340, 13724, 11614, 11246, 16556, 10176,\n",
            "         10109, 58760, 10139, 71777]])\n",
            "m a t h é m a t i q u e s\n",
            "tensor([[  149,   100, 10164, 10240, 13340, 13724, 11614, 11246, 16556, 10176,\n",
            "         10109, 58760, 10139, 71777, 10355]])\n",
            "q u i\n",
            "tensor([[  149,   100, 10164, 10240, 13340, 13724, 11614, 11246, 16556, 10176,\n",
            "         10109, 58760, 10139, 71777, 10355,   187]])\n",
            "s\n",
            "tensor([[  149,   100, 10164, 10240, 13340, 13724, 11614, 11246, 16556, 10176,\n",
            "         10109, 58760, 10139, 71777, 10355,   187,   112]])\n",
            "'\n",
            "tensor([[  149,   100, 10164, 10240, 13340, 13724, 11614, 11246, 16556, 10176,\n",
            "         10109, 58760, 10139, 71777, 10355,   187,   112, 26391]])\n",
            "i n t\n",
            "tensor([[  149,   100, 10164, 10240, 13340, 13724, 11614, 11246, 16556, 10176,\n",
            "         10109, 58760, 10139, 71777, 10355,   187,   112, 26391, 20017]])\n",
            "# # é r e\n",
            "tensor([[  149,   100, 10164, 10240, 13340, 13724, 11614, 11246, 16556, 10176,\n",
            "         10109, 58760, 10139, 71777, 10355,   187,   112, 26391, 20017, 12818]])\n",
            "# # s s e\n",
            "tensor([[  149,   100, 10164, 10240, 13340, 13724, 11614, 11246, 16556, 10176,\n",
            "         10109, 58760, 10139, 71777, 10355,   187,   112, 26391, 20017, 12818,\n",
            "         10754]])\n",
            "a u x\n",
            "tensor([[  149,   100, 10164, 10240, 13340, 13724, 11614, 11246, 16556, 10176,\n",
            "         10109, 58760, 10139, 71777, 10355,   187,   112, 26391, 20017, 12818,\n",
            "         10754, 84355]])\n",
            "e s p a c e s\n",
            "tensor([[  149,   100, 10164, 10240, 13340, 13724, 11614, 11246, 16556, 10176,\n",
            "         10109, 58760, 10139, 71777, 10355,   187,   112, 26391, 20017, 12818,\n",
            "         10754, 84355, 46514]])\n",
            "v e c t o r\n",
            "tensor([[  149,   100, 10164, 10240, 13340, 13724, 11614, 11246, 16556, 10176,\n",
            "         10109, 58760, 10139, 71777, 10355,   187,   112, 26391, 20017, 12818,\n",
            "         10754, 84355, 46514, 19428]])\n",
            "# # i e l\n",
            "tensor([[  149,   100, 10164, 10240, 13340, 13724, 11614, 11246, 16556, 10176,\n",
            "         10109, 58760, 10139, 71777, 10355,   187,   112, 26391, 20017, 12818,\n",
            "         10754, 84355, 46514, 19428, 10107]])\n",
            "# # s\n",
            "tensor([[  149,   100, 10164, 10240, 13340, 13724, 11614, 11246, 16556, 10176,\n",
            "         10109, 58760, 10139, 71777, 10355,   187,   112, 26391, 20017, 12818,\n",
            "         10754, 84355, 46514, 19428, 10107, 10131]])\n",
            "e t\n",
            "tensor([[  149,   100, 10164, 10240, 13340, 13724, 11614, 11246, 16556, 10176,\n",
            "         10109, 58760, 10139, 71777, 10355,   187,   112, 26391, 20017, 12818,\n",
            "         10754, 84355, 46514, 19428, 10107, 10131, 10754]])\n",
            "a u x\n",
            "tensor([[  149,   100, 10164, 10240, 13340, 13724, 11614, 11246, 16556, 10176,\n",
            "         10109, 58760, 10139, 71777, 10355,   187,   112, 26391, 20017, 12818,\n",
            "         10754, 84355, 46514, 19428, 10107, 10131, 10754, 44510]])\n",
            "t r a n s f o r m a t i o n\n",
            "tensor([[  149,   100, 10164, 10240, 13340, 13724, 11614, 11246, 16556, 10176,\n",
            "         10109, 58760, 10139, 71777, 10355,   187,   112, 26391, 20017, 12818,\n",
            "         10754, 84355, 46514, 19428, 10107, 10131, 10754, 44510, 10107]])\n",
            "# # s\n",
            "tensor([[  149,   100, 10164, 10240, 13340, 13724, 11614, 11246, 16556, 10176,\n",
            "         10109, 58760, 10139, 71777, 10355,   187,   112, 26391, 20017, 12818,\n",
            "         10754, 84355, 46514, 19428, 10107, 10131, 10754, 44510, 10107, 11614]])\n",
            "l i\n",
            "[CLS] L [UNK] algèbre linéaire est la branche des mathématiques qui s'intéresse aux espaces vectoriels et aux transformations li\n"
          ]
        }
      ]
    }
  ]
}