{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNe1VoD/gkWy1gx8pVcR6V7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pinnouse/zeroshot-unsurpervised-mt/blob/main/CSC413_FP_Data_Processing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Packages"
      ],
      "metadata": {
        "id": "jPxklVAAeuFQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "duN-zlNXePnh"
      },
      "outputs": [],
      "source": [
        "!pip install apache_beam mwparserfromhell\n",
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install ftfy regex tqdm\n",
        "!pip install git+https://github.com/openai/CLIP.git\n",
        "\n",
        "# multilingual CLIP pretrained\n",
        "# https://github.com/FreddeFrallan/Multilingual-CLIP\n",
        "!pip install multilingual-clip\n",
        "!pip install -U sentence-transformers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Batching"
      ],
      "metadata": {
        "id": "AXSWxhueemn_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "import random\n",
        "\n",
        "dataset = load_dataset(\"wikipedia\", \"20220301.simple\")\n",
        "dataset_fr = load_dataset(\"wikipedia\", \"20220301.fr\")\n",
        "\n",
        "training_percent = 0.8\n",
        "validation_percent = 0.1\n",
        "testing_percent = 0.1\n",
        "\n",
        "# english\n",
        "ds_en_len = len(dataset)\n",
        "test_index_en = ds_en_len * training_percent\n",
        "valid_index_en = ds_en_len * (training_percent + validation_percent)\n",
        "\n",
        "# start to test index\n",
        "test_data_en = dataset['train'][: int(test_index_en)]\n",
        "# test index to validation index\n",
        "validation_data_en = dataset['train'][int(test_index_en):int(valid_index_en)]\n",
        "# validation index to end\n",
        "test_data_en = dataset['train'][int(valid_index_en):]\n",
        "\n",
        "# french\n",
        "ds_fr_len = len(dataset_fr)\n",
        "test_index_fr = ds_fr_len * training_percent\n",
        "valid_index_fr = ds_fr_len * (training_percent + validation_percent)\n",
        "\n",
        "# start to test index\n",
        "test_data_fr = dataset['train'][: int(test_index_fr)]\n",
        "# test index to validation index\n",
        "validation_data_fr = dataset['train'][int(test_index_fr):int(valid_index_fr)]\n",
        "# validation index to end\n",
        "test_data_fr = dataset['train'][int(valid_index_fr):]\n",
        "\n",
        "def batch_loader(dataset, batch_size, shuffle=True):\n",
        "  text = dataset['train']['text']\n",
        "\n",
        "  if shuffle:\n",
        "    random.shuffle(text)\n",
        "\n",
        "  data_batch = []\n",
        "\n",
        "  for i in range((len(text) // batch_size)):\n",
        "    data_batch.append(text[i * batch_size:(i + 1) * batch_size])\n",
        "\n",
        "  if len(text) % batch_size != 0:\n",
        "    data_batch.append(text[(len(text) // batch_size) * batch_size:])\n",
        "  \n",
        "  return data_batch"
      ],
      "metadata": {
        "id": "ZAISBqDhehqY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = map(lambda x: x['text'].replace(\"\\n\", ' ').split(\". \"), dataset['train'])\n",
        "text_long = []\n",
        "for t in text:\n",
        "  for s in t:\n",
        "    text_long.append(s) \n",
        "\n",
        "text_fr = map(lambda x: x['text'].replace(\"\\n\", ' ').split(\". \"), dataset_fr['train'])\n",
        "for t in text_fr:\n",
        "  print(t)\n",
        "  break"
      ],
      "metadata": {
        "id": "WsRUya-BfQL1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchtext\n",
        "import clip\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "# model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
        "\n",
        "text_model = SentenceTransformer('sentence-transformers/clip-ViT-B-32-multilingual-v1',\n",
        "                                 device=device)\n",
        "\n",
        "context_length = 64\n",
        "#glove = torchtext.vocab.GloVe(name=\"6B\", dim=50)\n",
        "#ft = torchtext.vocab.FastText(language=\"simple\")"
      ],
      "metadata": {
        "id": "Tjde4ItSfn7S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenize/Embbed English data"
      ],
      "metadata": {
        "id": "dZB1LMezgmpv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_en = []\n",
        "for (i, t) in enumerate(text_long):\n",
        "  if i > 5:\n",
        "    break\n",
        "  tokenized = tokenizer(t, padding='max_length', max_length=64, return_tensors='pt').input_ids[0] #took 18 mins to run\n",
        "  if len(tokenized) <= 64:\n",
        "    sentences = []\n",
        "    for s in range(len(tokenized)):\n",
        "      sentences.append(tokenizer.decode(tokenized[1:s], skip_special_tokens=True))\n",
        "    #[bs x 64 x 512]\n",
        "    clips = text_model.encode(sentences).detach()\n",
        "    train_data_en.append((t,clips,tokenized))"
      ],
      "metadata": {
        "id": "F3XxRAS8gRH7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenize/Embbed French data"
      ],
      "metadata": {
        "id": "OhNwN44QggpK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_fr = []\n",
        "\n",
        "for (i, t) in enumerate(text_fr):\n",
        "  if i > 5:\n",
        "    break\n",
        "  sentence = t[0]\n",
        "  if len(sentence.split(' ')) > 60:\n",
        "    continue\n",
        "  tokenized = tokenizer(sentence, padding='max_length', max_length=64, return_tensors='pt')['input_ids'][0]\n",
        "  # tokenized = tokenized.to(device)\n",
        "  train_data_fr.append((sentence, tokenized))"
      ],
      "metadata": {
        "id": "EljCs2UJghH2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}